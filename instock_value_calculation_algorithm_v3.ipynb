{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af9f421-3b6b-45f8-a4ea-e56a7b0dcdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install openjdk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d16438-167a-4d37-b05a-a74116e598cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.2.0 in /opt/conda/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.7/site-packages (from pyspark==3.2.0) (0.10.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcef377-31d1-4cf3-95d8-44251d0f0b25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9383963-fa20-43e3-bd0f-e1d7d0a65f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "import datetime as DT\n",
    "from datetime import date, timedelta as td\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faa641-9621-4e6c-94f1-747dc98bb061",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e3d0b9-74be-48a9-98c4-7d65bb0f4c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. CP data\n",
    "def read_cp_data(path):\n",
    "    \n",
    "    \n",
    "    df_cp = spark.read.parquet(path)\n",
    "    \n",
    "    df_cp = df_cp.withColumn(\"PRODUCT_PART_NUMBER_IN_CP\", F.col(\"PRODUCT_PART_NUMBER\")) \\\n",
    "            .withColumn(\"ORDER_PLACED_DTTM_CP\", F.col(\"ORDER_PLACED_DTTM\"))\\\n",
    "\t\t\t.withColumn(\"ORDER_PLACED_YEAR_CP\", F.year(F.col(\"ORDER_PLACED_DTTM\"))) \\\n",
    "\t\t\t.withColumn(\"ORDER_PLACED_MONTH_CP\", F.month(F.col(\"ORDER_PLACED_DTTM\")))\n",
    "    \n",
    "    df_cp = df_cp.select(\"CUSTOMER_ID\", \"ORDER_ID\", \"ORDER_LINE_ID\",\n",
    "                               \"PRODUCT_PART_NUMBER_IN_CP\",\n",
    "                               \"ORDER_PLACED_DTTM_CP\",\n",
    "                               \"ORDER_PLACED_YEAR_CP\",\n",
    "                               \"ORDER_PLACED_MONTH_CP\",\n",
    "                               \"LOCATION_CODE\",\n",
    "                               \"ORDER_LINE_QUANTITY\",\n",
    "                               \"ORDER_STATUS\",\n",
    "                               \"IS_AUTO_SHIP\",\n",
    "                               \"IB_COST\",\"OB_COST\", \"COGS\", \"AVERAGE_PRICE\", \"NET_MARGIN\")\n",
    "    \n",
    "    return df_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3e3496-0bdb-4215-891f-76bdfc6d3c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Complete clickstream data\n",
    "def read_clickstream_data(path):\n",
    "    \n",
    "    df_clickstream_data = spark.read.parquet(path)\n",
    "\n",
    "    df_clickstream_data = df_clickstream_data.withColumn(\"SESSION_YEAR\",F.year(\"SESSION_EST_TIMESTAMP\"))\\\n",
    "                               .withColumn(\"SESSION_MONTH\",F.month(\"SESSION_EST_TIMESTAMP\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_clickstream_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a5c2d1-ee84-4ef8-a93f-a2a5162dd00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Complete scheduled as order data\n",
    "def read_as_orders_data(path):\n",
    "    \n",
    "    df_as_orders_data = spark.read.parquet(path)\n",
    "\n",
    "    df_as_orders_data = df_as_orders_data.withColumn(\"ORDER_PLACED_DATE\",F.to_date(\"ORDER_PLACED_DTTM\"))\\\n",
    "                                          .withColumn(\"ORDER_PLACED_YEAR\",F.year(\"ORDER_PLACED_DTTM\"))\\\n",
    "                                          .withColumn(\"ORDER_PLACED_MONTH\",F.month(\"ORDER_PLACED_DTTM\"))\n",
    "    \n",
    "    return df_as_orders_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a828d74-9f37-4477-8413-87c65c717ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. SKU-Month pool\n",
    "def read_sku_month_candidate_data(path):\n",
    "    \n",
    "    pd_sku_month = pd.read_csv(path)\n",
    "    \n",
    "    return pd_sku_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c116e792-6c84-4947-a221-0123e852c7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Clickstream slice\n",
    "def clickstream_slice_pandas(df_data, sample_sku, sample_year, sample_month):\n",
    "    \n",
    "    df_slice = df_data.where((F.col(\"PRODUCT_PART_NUMBER\")== str(sample_sku))\\\n",
    "                                            & (F.col(\"SESSION_YEAR\") == sample_year) \\\n",
    "                                            & (F.col(\"SESSION_MONTH\") == sample_month))\n",
    "    \n",
    "    pd_slice = df_slice.toPandas()\n",
    "    \n",
    "    return df_slice, pd_slice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e15127-8a52-4d53-898c-26efebe0d8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. Clickstream count summaries\n",
    "\n",
    "def clickstream_counts_by_instock_status(pd_clickstream_data_slice):\n",
    "    \n",
    "    clickstream_identifier_variables = [\"CUSTOMER_ID\",\n",
    "                              \"SESSION_DATE\",\n",
    "                              \"SESSION_EST_TIMESTAMP\",\n",
    "                              \"PRODUCT_PART_NUMBER\",\n",
    "                              \"STATUS\",\n",
    "                              \"DEVICE_CATEGORY\"]\n",
    "\n",
    "    is_clickstream_counts = len(pd_clickstream_data_slice[pd_clickstream_data_slice[\"STATUS\"] == \"IN STOCK\"][clickstream_identifier_variables].drop_duplicates())\n",
    "    oos_clickstream_counts = len(pd_clickstream_data_slice[pd_clickstream_data_slice[\"STATUS\"] == \"OUT OF STOCK\"][clickstream_identifier_variables].drop_duplicates())\n",
    "    total_clickstream_counts = len(pd_clickstream_data_slice[clickstream_identifier_variables].drop_duplicates())\n",
    "    \n",
    "    return is_clickstream_counts, oos_clickstream_counts, total_clickstream_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f29bdcd5-fea7-42fc-a50d-a3690596b978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Units sold summaries\n",
    "def units_sold_by_as_status(df_cp_sku_month_data):\n",
    "    \n",
    "    non_as_units_sold = df_cp_sku_month_data.filter(F.col(\"IS_AUTO_SHIP\") == \"NON AUTO SHIP\").select(F.sum(\"ORDER_LINE_QUANTITY\")).collect()[0][0]\n",
    "    as_units_sold = df_cp_sku_month_data.filter(F.col(\"IS_AUTO_SHIP\").isin([\"AUTO SHIP\", \"AUTO SHIP NOW\"])).select(F.sum(\"ORDER_LINE_QUANTITY\")).collect()[0][0]\n",
    "    total_units_sold = df_cp_sku_month_data.select(F.sum(\"ORDER_LINE_QUANTITY\")).collect()[0][0]\n",
    "\n",
    "    \n",
    "    return non_as_units_sold, as_units_sold, total_units_sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81803ac-8a4b-4ed9-a6b9-efbf2561c7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. Find clickstream cutoff dates\n",
    "\n",
    "def find_clickstream_cutoff_dates(pd_clickstream_data_slice):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # 1. Dummy variables that indicate IS and OOS\n",
    "    \n",
    "    pd_clickstream_data_slice[\"IS\"] = np.where(pd_clickstream_data_slice[\"STATUS\"] == \"IN STOCK\", 1, 0)\n",
    "    pd_clickstream_data_slice[\"OOS\"] = np.where(pd_clickstream_data_slice[\"STATUS\"] == \"OUT OF STOCK\", 1, 0)\n",
    "    \n",
    "    pd_clickstream_is_oos_counts_by_date = pd_clickstream_data_slice.groupby([\"SESSION_DATE\"])[[\"IS\", \"OOS\"]].sum().reset_index()\n",
    "    pd_clickstream_is_oos_counts_by_date[\"IS_ADJ\"] = np.where(pd_clickstream_is_oos_counts_by_date[\"IS\"]/(pd_clickstream_is_oos_counts_by_date[\"IS\"]+pd_clickstream_is_oos_counts_by_date[\"OOS\"])<0.1, 0, pd_clickstream_is_oos_counts_by_date[\"IS\"]) \n",
    "    pd_clickstream_is_oos_counts_by_date[\"OOS_ADJ\"] = np.where(pd_clickstream_is_oos_counts_by_date[\"OOS\"]/(pd_clickstream_is_oos_counts_by_date[\"IS\"]+pd_clickstream_is_oos_counts_by_date[\"OOS\"])<0.1, 0, pd_clickstream_is_oos_counts_by_date[\"OOS\"]) \n",
    "    \n",
    "    # 2. Generate status switching variable\n",
    "\n",
    "    N = len(pd_clickstream_is_oos_counts_by_date)\n",
    "\n",
    "    for row in range(N):\n",
    "\n",
    "        if row == 0:\n",
    "\n",
    "            temp = pd_clickstream_is_oos_counts_by_date.iloc[row:(row+1),].reset_index()\n",
    "            is_counts = temp[\"IS_ADJ\"][0]\n",
    "            oos_counts = temp[\"OOS_ADJ\"][0]\n",
    "            \n",
    "            # initialization for the first date in this monthly sample \n",
    "            if (is_counts == 0) & (oos_counts == 0):\n",
    "                temp_status = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0):\n",
    "                temp_status = \"IS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0):\n",
    "                temp_status = \"OOS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0):\n",
    "                temp_status = \"SWITCHING TO IS\"\n",
    "\n",
    "            temp[\"SWITCHING_STATUS\"] = temp_status\n",
    "\n",
    "            pd_clickstream_cutoff_point_detection = temp\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp_t_minus_1 = pd_clickstream_cutoff_point_detection.iloc[(row-1):row,].reset_index()\n",
    "            switching_status_t_minus_1 = temp_t_minus_1[\"SWITCHING_STATUS\"][0]\n",
    "\n",
    "            temp_t = pd_clickstream_is_oos_counts_by_date.iloc[row:(row+1),].reset_index()\n",
    "            is_counts = temp_t[\"IS_ADJ\"][0]\n",
    "            oos_counts = temp_t[\"OOS_ADJ\"][0]\n",
    "            \n",
    "            if (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"    \n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"OOS\"    \n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"OOS\"\n",
    "\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"OOS\"    \n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "\n",
    "            else:\n",
    "                switching_status_t = np.nan\n",
    "                print(\"There is NULL value in the table.\")\n",
    "\n",
    "            temp_t[\"SWITCHING_STATUS\"] = switching_status_t\n",
    "            \n",
    "            pd_clickstream_cutoff_point_detection = pd_clickstream_cutoff_point_detection.append(temp_t)\n",
    "\n",
    "\n",
    "    pd_clickstream_cutoff_points = pd_clickstream_cutoff_point_detection[pd_clickstream_cutoff_point_detection[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"])][[\"SESSION_DATE\",\"SWITCHING_STATUS\"]]\n",
    "    \n",
    "    \n",
    "    return pd_clickstream_cutoff_point_detection, pd_clickstream_cutoff_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9dd260-4c08-4765-b138-f7f5093daa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Clickstream RDD customer list \n",
    "def find_clickstream_rdd_relevant_customers(pd_clickstream_cutoff_points, pd_clickstream_cutoff_point_detection, pd_clickstream_data_slice, sufficiency_ratio, initial_window_length, first_layer_index):\n",
    "    \n",
    "    \n",
    "    # print(\"==========CONSTRUCTING CLICKSTREAM RDD CUSTOMER SAMPLE FOR \" + first_layer_index + \" ==========\")\n",
    "    \n",
    "    # clickstream_rdd_customers_temp = pd.DataFrame(columns = [\"CUSTOMER_ID\"])\n",
    "    \n",
    "    clickstream_rdd_customers_temp = []\n",
    "\n",
    "    # 1. RDD sample generation\n",
    "    for k in range(len(pd_clickstream_cutoff_points)):\n",
    "        \n",
    "        window_length = initial_window_length\n",
    "\n",
    "        cutoff_point = pd_clickstream_cutoff_points.iloc[k,][\"SESSION_DATE\"]\n",
    "\n",
    "        cutoff_point_name = pd_clickstream_cutoff_points.iloc[k,][\"SWITCHING_STATUS\"]\n",
    "\n",
    "        # Check T days prior to the cutoff date and T days after the cutoff date\n",
    "        while window_length > 0:\n",
    "\n",
    "            # Window start date\n",
    "            window_left_end = max(cutoff_point - DT.timedelta(days=window_length), min(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"]))\n",
    "            # Window end date\n",
    "            window_right_end = min(cutoff_point + DT.timedelta(days=(window_length+1)), max(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"])+ DT.timedelta(days=1))\n",
    "\n",
    "            pd_cutoff_point_detection_rdd_sample_temp = pd_clickstream_cutoff_point_detection[(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"] >= window_left_end) & (pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"] < window_right_end) ]\n",
    "\n",
    "            # [1] We want to make sure this sample only has the cutoff point that is under inspection\n",
    "            if np.sum(pd_cutoff_point_detection_rdd_sample_temp[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"]))>1:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:\n",
    "                    temp=0\n",
    "                #   print(\"THIS IS THE SHORTEST RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT IS STILL NOT CLEAN\")\n",
    "                continue\n",
    "            else:\n",
    "                temp=0\n",
    "                # print(\"FOUND A CLEAN RDD SAMPLE FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"CHECKING DATA SUFFICIENCY\")\n",
    "\n",
    "            # [1] Clickstream RDD horizon\n",
    "            clickstream_rdd_sample_temp = pd_clickstream_data_slice[ (pd_clickstream_data_slice[\"SESSION_DATE\"]>=window_left_end) & (pd_clickstream_data_slice[\"SESSION_DATE\"]< window_right_end)]\n",
    "\n",
    "            # [2] Remove duplicate clickstream data points   \n",
    "            clickstream_identifier_variables = [\"CUSTOMER_ID\", \"SESSION_EST_TIMESTAMP\",\n",
    "                            \"PRODUCT_PART_NUMBER\", \n",
    "                            \"STATUS\", \n",
    "                            \"DEVICE_CATEGORY\",\n",
    "                            \"SESSION_DATE\", \"SESSION_YEAR\", \"SESSION_MONTH\"]\n",
    "            clickstream_rdd_sample_cleaned = clickstream_rdd_sample_temp[clickstream_identifier_variables].drop_duplicates()\n",
    "\n",
    "            # [3] Ensure at least 50% of days on both sides of the cutoff point need to have data \n",
    "            # Summarize daily data points\n",
    "            clickstream_counts_distribution_across_days = clickstream_rdd_sample_cleaned.groupby([\"SESSION_DATE\"])[\"CUSTOMER_ID\"].count().reset_index()\n",
    "\n",
    "            if (clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] < cutoff_point][\"SESSION_DATE\"].nunique() >= window_length*sufficiency_ratio) & \\\n",
    "             (clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] > cutoff_point][\"SESSION_DATE\"].nunique() >= window_length*sufficiency_ratio):\n",
    "                # print(\"WE FOUND SUFFICIENT DATA FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"LEFT SIDE HAS \" + str(clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] < cutoff_point][\"SESSION_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"RIGHT SIDE HAS \" + str(clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] > cutoff_point][\"SESSION_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"THIS RDD SAMPLE IS COLLECTED FOR REGRESSION AND ANALYSES\")\n",
    "                temp = 0\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:   \n",
    "                    # print(\"THIS IS THE SHORTEST RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT DOES NOT HAVE SUFFICIENT DATA\")\n",
    "                    temp = 0\n",
    "                continue\n",
    "\n",
    "        if window_length == 0:\n",
    "            # print(\"UNFORTUNATELY, WE CANNOT FIND AN IDEAL RDD SAMPLE FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            temp = 0\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # print(\"WE HAVE FOUND THE RDD SAMPLE FOR REGRESSION FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            # print(\"THE LEFT END DATE IS: \" + str(window_left_end))\n",
    "            # print(\"THE RIGHT END DATE IS: \" + str(window_right_end))\n",
    "            temp = 0\n",
    "            \n",
    "            # [1] Collect the customers\n",
    "\n",
    "            # clickstream_rdd_customers_temp0 = clickstream_rdd_sample_cleaned[[\"CUSTOMER_ID\"]].drop_duplicates().reset_index()[[\"CUSTOMER_ID\"]]\n",
    "            \n",
    "            clickstream_rdd_customers_temp0 = clickstream_rdd_sample_cleaned[[\"CUSTOMER_ID\"]].drop_duplicates()[\"CUSTOMER_ID\"].to_list()\n",
    "            \n",
    "            second_layer_index = str(cutoff_point) + \"_\" + str(cutoff_point_name) + \"_\" + str(window_length)\n",
    "\n",
    "            # print(\"(CLICKSTREAM) THE NUMBER OF CUSTOMERS IN \" + first_layer_index + \" SAMPLE \" + second_layer_index + \" IS \" + str(len(clickstream_rdd_customers_temp0)))\n",
    "\n",
    "            # clickstream_rdd_customers_temp = clickstream_rdd_customers_temp.append(clickstream_rdd_customers_temp0).drop_duplicates().reset_index()[[\"CUSTOMER_ID\"]]\n",
    "            clickstream_rdd_customers_temp = clickstream_rdd_customers_temp + clickstream_rdd_customers_temp0\n",
    "            clickstream_rdd_customers_temp = [*set(clickstream_rdd_customers_temp)]\n",
    "    \n",
    "    # print(\"(CLICKSTREAM) THE NUMBER OF CUSTOMERS IN \" + first_layer_index + \" IS \" + str(len(clickstream_rdd_customers_temp)))\n",
    "\n",
    "    return clickstream_rdd_customers_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99f6f84-72bb-4918-bf6e-4e9f511bcc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10. Clickstream RDD sample construction\n",
    "def construct_clickstream_rdd_samples(pd_clickstream_cutoff_points, pd_clickstream_cutoff_point_detection, pd_clickstream_data_slice, sufficiency_ratio, initial_window_length, first_layer_index, df_cp):\n",
    "    \n",
    "    \n",
    "    print(\"==========CONSTRUCTING CLICKSTREAM RDD SAMPLE FOR \" + first_layer_index + \" ==========\")\n",
    "    \n",
    "    \n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    clickstream_rdd_sample_collection_temp = {}\n",
    "    \n",
    "    # 1. RDD sample generation\n",
    "    for k in range(len(pd_clickstream_cutoff_points)):\n",
    "        \n",
    "        window_length = initial_window_length\n",
    "\n",
    "        cutoff_point = pd_clickstream_cutoff_points.iloc[k,][\"SESSION_DATE\"]\n",
    "\n",
    "        cutoff_point_name = pd_clickstream_cutoff_points.iloc[k,][\"SWITCHING_STATUS\"]\n",
    "\n",
    "        # Check T days prior to the cutoff date and T days after the cutoff date\n",
    "        while window_length > 0:\n",
    "\n",
    "            # Window start date\n",
    "            window_left_end = max(cutoff_point - DT.timedelta(days=window_length), min(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"]))\n",
    "            # Window end date\n",
    "            window_right_end = min(cutoff_point + DT.timedelta(days=(window_length+1)), max(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"])+ DT.timedelta(days=1))\n",
    "\n",
    "            pd_cutoff_point_detection_rdd_sample_temp = pd_clickstream_cutoff_point_detection[(pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"] >= window_left_end) & (pd_clickstream_cutoff_point_detection[\"SESSION_DATE\"] < window_right_end) ]\n",
    "\n",
    "            # [1] We want to make sure this sample only has the cutoff point that is under inspection\n",
    "            if np.sum(pd_cutoff_point_detection_rdd_sample_temp[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"]))>1:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:\n",
    "                    # print(\"THIS IS THE SHORTEST RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT IS STILL NOT CLEAN\")\n",
    "                    temp = 1\n",
    "                continue \n",
    "            else:\n",
    "                # print(\"FOUND A CLEAN RDD SAMPLE FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"CHECKING DATA SUFFICIENCY\")\n",
    "                temp = 1\n",
    "\n",
    "            # [1] Clickstream RDD horizon\n",
    "            clickstream_rdd_sample_temp = pd_clickstream_data_slice[ (pd_clickstream_data_slice[\"SESSION_DATE\"]>=window_left_end) & (pd_clickstream_data_slice[\"SESSION_DATE\"]< window_right_end)]\n",
    "\n",
    "            # [2] Remove duplicate clickstream data points   \n",
    "            clickstream_identifier_variables = [\"CUSTOMER_ID\", \"SESSION_EST_TIMESTAMP\",\n",
    "                            \"PRODUCT_PART_NUMBER\", \n",
    "                            \"STATUS\", \n",
    "                            \"DEVICE_CATEGORY\",\n",
    "                            \"SESSION_DATE\", \"SESSION_YEAR\", \"SESSION_MONTH\"]\n",
    "            clickstream_rdd_sample_cleaned = clickstream_rdd_sample_temp[clickstream_identifier_variables].drop_duplicates()\n",
    "\n",
    "            # [3] Ensure at least 50% of days on both sides of the cutoff point need to have data \n",
    "            # Summarize daily data points\n",
    "            clickstream_counts_distribution_across_days = clickstream_rdd_sample_cleaned.groupby([\"SESSION_DATE\"])[\"CUSTOMER_ID\"].count().reset_index()\n",
    "\n",
    "            if (clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] < cutoff_point][\"SESSION_DATE\"].nunique() >= window_length*sufficiency_ratio) & \\\n",
    "             (clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] > cutoff_point][\"SESSION_DATE\"].nunique() >= window_length*sufficiency_ratio):\n",
    "                # print(\"WE FOUND SUFFICIENT DATA FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"LEFT SIDE HAS \" + str(clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] < cutoff_point][\"SESSION_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"RIGHT SIDE HAS \" + str(clickstream_counts_distribution_across_days[clickstream_counts_distribution_across_days[\"SESSION_DATE\"] > cutoff_point][\"SESSION_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"THIS RDD SAMPLE IS COLLECTED FOR REGRESSION AND ANALYSES\")\n",
    "                temp = 1\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:   \n",
    "                    # print(\"THIS IS THE SHORTEST RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT DOES NOT HAVE SUFFICIENT DATA\")\n",
    "                    temp = 1\n",
    "                continue\n",
    "\n",
    "        if window_length == 0:\n",
    "            # print(\"UNFORTUNATELY, WE CANNOT FIND AN IDEAL RDD SAMPLE FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            temp = 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # print(\"WE HAVE FOUND THE RDD SAMPLE FOR REGRESSION FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            # print(\"THE LEFT END DATE IS: \" + str(window_left_end))\n",
    "            # print(\"THE RIGHT END DATE IS: \" + str(window_right_end))\n",
    "            temp = 1\n",
    "\n",
    "            # 2. Feature Engineering\n",
    "\n",
    "            # [1] The precise timestamp of the cutoff point\n",
    "            \n",
    "            # We make the following update: if the status switches from OOS to IS then we use the first IS timestamp within the cutoff date as the cutoff timestamp. Similar for IS -> OOS.  \n",
    "            \n",
    "            clickstream_rdd_sample_on_cutoff_date = clickstream_rdd_sample_cleaned[clickstream_rdd_sample_cleaned[\"SESSION_DATE\"] == cutoff_point]\n",
    "            \n",
    "            clickstream_rdd_sample_on_cutoff_date[\"STATUS_RANK\"] = clickstream_rdd_sample_on_cutoff_date.groupby(\"STATUS\")[\"SESSION_EST_TIMESTAMP\"].rank(method = \"dense\", ascending = True).astype(int)\n",
    "\n",
    "            if cutoff_point_name == \"SWITCHING TO IS\":\n",
    "\n",
    "                cutoff_point_timestamp = clickstream_rdd_sample_on_cutoff_date[clickstream_rdd_sample_on_cutoff_date[\"STATUS_RANK\"]==1][clickstream_rdd_sample_on_cutoff_date[\"STATUS\"] == \"IN STOCK\"].reset_index().iloc[0][\"SESSION_EST_TIMESTAMP\"]\n",
    "\n",
    "            else:\n",
    "\n",
    "                cutoff_point_timestamp = clickstream_rdd_sample_on_cutoff_date[clickstream_rdd_sample_on_cutoff_date[\"STATUS_RANK\"]==1][clickstream_rdd_sample_on_cutoff_date[\"STATUS\"] == \"OUT OF STOCK\"].reset_index().iloc[0][\"SESSION_EST_TIMESTAMP\"]\n",
    "\n",
    "                \n",
    "            # [2] Treatment variable    \n",
    "            \n",
    "            clickstream_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] = np.where(clickstream_rdd_sample_cleaned[\"STATUS\"] == \"IN STOCK\", 1, 0)\n",
    "\n",
    "\n",
    "            # [3] Generate the hour difference between the timestamp and the cutoff point\n",
    "            \n",
    "            # We make the following update: We are going to drop data points that are \"not consistent with RDD design\"\n",
    "           \n",
    "            clickstream_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] = (clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP\"] - cutoff_point_timestamp).dt.total_seconds()/3600\n",
    "\n",
    "            if cutoff_point_name == \"SWITCHING TO IS\":\n",
    "\n",
    "                clickstream_rdd_sample_cleaned[\"DROP_INDICATOR\"] = np.where( ((clickstream_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 1) & (clickstream_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] < 0)) \\\n",
    "                                                                              | ((clickstream_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 0) & (clickstream_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] >= 0)), 1, 0)\n",
    "            \n",
    "                # print(\"BEFORE DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(clickstream_rdd_sample_cleaned)))\n",
    "                \n",
    "                clickstream_rdd_sample_cleaned = clickstream_rdd_sample_cleaned[clickstream_rdd_sample_cleaned[\"DROP_INDICATOR\"]==0]\n",
    "                \n",
    "                # print(\"AFTER DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(clickstream_rdd_sample_cleaned)))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                clickstream_rdd_sample_cleaned[\"DROP_INDICATOR\"] = np.where( ((clickstream_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 1) & (clickstream_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] >= 0)) \\\n",
    "                                                                              | ((clickstream_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 0) & (clickstream_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] < 0)), 1, 0)\n",
    "\n",
    "                # print(\"BEFORE DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(clickstream_rdd_sample_cleaned)))\n",
    "                \n",
    "                clickstream_rdd_sample_cleaned = clickstream_rdd_sample_cleaned[clickstream_rdd_sample_cleaned[\"DROP_INDICATOR\"]==0]\n",
    "                \n",
    "                # print(\"AFTER DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(clickstream_rdd_sample_cleaned)))\n",
    "            \n",
    "            # [4] Outcome Variable Construction\n",
    "\n",
    "            customer_id_list_in_clickstream_rdd_sample = clickstream_rdd_sample_cleaned[[\"CUSTOMER_ID\"]].drop_duplicates()[\"CUSTOMER_ID\"].to_list()\n",
    "\n",
    "            # Add filtering variables\n",
    "            clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP_UB\"] = clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP\"] + pd.Timedelta(hours=24)\n",
    "            clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP_END\"] = clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP\"] + pd.Timedelta(hours=24*365)\n",
    "\n",
    "            # Max timestamp possible for CP calculation\n",
    "            max_timestamp = np.max(clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP_END\"])\n",
    "\n",
    "            # Min timestamp possible for CP calculation\n",
    "            min_timestamp = np.min(clickstream_rdd_sample_cleaned[\"SESSION_EST_TIMESTAMP\"])\n",
    "\n",
    "            # Focus on customers in clickstream data & Focus on time period that is relevant\n",
    "            df_cp_clickstream = df_cp.filter( F.col(\"CUSTOMER_ID\").isin(customer_id_list_in_clickstream_rdd_sample)) \n",
    "                                             \n",
    "                                             # & \\\n",
    "                                             # (F.col(\"ORDER_PLACED_DTTM\") >= min_timestamp) & \\\n",
    "                                             # (F.col(\"ORDER_PLACED_DTTM\") <= max_timestamp) )\n",
    "\n",
    "            # df_cp_clickstream = df_cp_clickstream.withColumn(\"NET_MARGIN_V2\",  F.col(\"AVERAGE_PRICE\")-F.col(\"IB_COST\")-F.col(\"OB_COST\")-F.col(\"COGS\"))\n",
    "\n",
    "            # Transform the small cp data into pandas dataframe\n",
    "\n",
    "            print(\"==========Converting CP data into Pandas data frame==========\")\n",
    "            start_time = time.time()\n",
    "            pd_cp_clickstream = df_cp_clickstream.toPandas()\n",
    "            pd_cp_clickstream[\"NET_MARGIN_V2\"] = pd_cp_clickstream[\"AVERAGE_PRICE\"] - pd_cp_clickstream[\"IB_COST\"] - pd_cp_clickstream[\"OB_COST\"] - pd_cp_clickstream[\"COGS\"]\n",
    "            pd_cp_clickstream[\"ORDER_LINE_QUANTITY\"] = pd_cp_clickstream[\"ORDER_LINE_QUANTITY\"].astype(float)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            # Columns we need from clickstream data\n",
    "            clickstream_relevant_columns = [\"CUSTOMER_ID\", \"SESSION_EST_TIMESTAMP\", \"SESSION_EST_TIMESTAMP_UB\", \"SESSION_EST_TIMESTAMP_END\",\n",
    "                            \"PRODUCT_PART_NUMBER\", \n",
    "                            \"STATUS\", \n",
    "                            \"DEVICE_CATEGORY\", \n",
    "                            \"SESSION_YEAR\", \"SESSION_MONTH\", \n",
    "                            \"RUNNING_VARIABLE\", \"TREATMENT_VARIABLE\"]\n",
    "            clickstream_rdd_sample_cleaned_adj = clickstream_rdd_sample_cleaned[clickstream_relevant_columns]\n",
    "\n",
    "            start_time_0 = time.time()\n",
    "\n",
    "            # alternative way\n",
    "            for row in range(len(clickstream_rdd_sample_cleaned_adj)):\n",
    "\n",
    "                # print(\"==========NOW GENERATING CP FOR DATA POINT: \" + str(row) + \" ==========\")\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                # Loop through each clickstream data point\n",
    "\n",
    "                clickstream_dp = clickstream_rdd_sample_cleaned_adj.iloc[row:(row+1),:].reset_index()\n",
    "\n",
    "                # info needed to be added to CP data\n",
    "                customer_id = clickstream_dp[\"CUSTOMER_ID\"][0]\n",
    "                product_part_number = clickstream_dp[\"PRODUCT_PART_NUMBER\"][0]\n",
    "                timestamp_lb = clickstream_dp[\"SESSION_EST_TIMESTAMP\"][0]\n",
    "                timestamp_ub = clickstream_dp[\"SESSION_EST_TIMESTAMP_UB\"][0]\n",
    "                timestamp_end = clickstream_dp[\"SESSION_EST_TIMESTAMP_END\"][0]\n",
    "                device_type = clickstream_dp[\"DEVICE_CATEGORY\"][0]\n",
    "                instock_status = clickstream_dp[\"STATUS\"][0]\n",
    "                running_variable = clickstream_dp[\"RUNNING_VARIABLE\"][0]\n",
    "                treatment_variable = clickstream_dp[\"TREATMENT_VARIABLE\"][0]\n",
    "\n",
    "                # get CP data that corresponds to the customer id of the clickstream dp\n",
    "                pd_cp_clickstream_dp = pd_cp_clickstream[pd_cp_clickstream[\"CUSTOMER_ID\"] == customer_id]\n",
    "\n",
    "                # add the info from the clickstream dp into CP data\n",
    "                pd_cp_clickstream_dp[\"PRODUCT_PART_NUMBER\"] = product_part_number\n",
    "                pd_cp_clickstream_dp[\"SESSION_EST_TIMESTAMP\"] = timestamp_lb\n",
    "                pd_cp_clickstream_dp[\"SESSION_EST_TIMESTAMP_UB\"] = timestamp_ub\n",
    "                pd_cp_clickstream_dp[\"SESSION_EST_TIMESTAMP_END\"] = timestamp_end\n",
    "                pd_cp_clickstream_dp[\"DEVICE_CATEGORY\"] = device_type\n",
    "                pd_cp_clickstream_dp[\"STATUS\"] = instock_status\n",
    "                pd_cp_clickstream_dp[\"RUNNING_VARIABLE\"] = running_variable\n",
    "                pd_cp_clickstream_dp[\"TREATMENT_VARIABLE\"] = treatment_variable\n",
    "\n",
    "                # if len(pd_cp_clickstream_dp) == 0:\n",
    "\n",
    "                #    print(\"THIS DATA POINT HAS NO CP RECORD\")\n",
    "\n",
    "                # else:\n",
    "\n",
    "                #    print(\"THIS DATA POINT HAS CP RECORD: \" + str(len(pd_cp_clickstream_dp)))\n",
    "\n",
    "\n",
    "                pd_cp_clickstream_dp[\"CP_INCLUDED\"] = np.where(\n",
    "\n",
    "                    ((pd_cp_clickstream_dp['PRODUCT_PART_NUMBER_IN_CP'] != pd_cp_clickstream_dp['PRODUCT_PART_NUMBER']) & (pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP']) & (pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP_UB']))\\\n",
    "                    | \\\n",
    "\n",
    "                    ((pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP_UB']) & (pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP_END']))\n",
    "\n",
    "                    , 1, 0)\n",
    "                \n",
    "                \n",
    "                pd_cp_clickstream_dp[\"CP_INCLUDED_V2\"] = np.where(\n",
    "\n",
    "                (pd_cp_clickstream_dp['PRODUCT_PART_NUMBER_IN_CP'] != pd_cp_clickstream_dp['PRODUCT_PART_NUMBER']) \\\n",
    "                 & ((pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP']) \\\n",
    "                    & (pd_cp_clickstream_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_clickstream_dp['SESSION_EST_TIMESTAMP_END']))\n",
    "\n",
    "                , 1, 0)\n",
    "\n",
    "                pd_cp_clickstream_dp[\"CP\"] = pd_cp_clickstream_dp[\"CP_INCLUDED\"] * pd_cp_clickstream_dp[\"NET_MARGIN\"] * pd_cp_clickstream_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                pd_cp_clickstream_dp[\"CP_V2\"] = pd_cp_clickstream_dp[\"CP_INCLUDED\"] * pd_cp_clickstream_dp[\"NET_MARGIN_V2\"] * pd_cp_clickstream_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "\n",
    "                pd_cp_clickstream_dp[\"CP_V3\"] = pd_cp_clickstream_dp[\"CP_INCLUDED_V2\"] * pd_cp_clickstream_dp[\"NET_MARGIN\"] * pd_cp_clickstream_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                pd_cp_clickstream_dp[\"CP_V4\"] = pd_cp_clickstream_dp[\"CP_INCLUDED_V2\"] * pd_cp_clickstream_dp[\"NET_MARGIN_V2\"] * pd_cp_clickstream_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "\n",
    "                \n",
    "                groupVars = [\"PRODUCT_PART_NUMBER\", \"CUSTOMER_ID\", \n",
    "                             \"SESSION_EST_TIMESTAMP\", \"SESSION_EST_TIMESTAMP_UB\", \"SESSION_EST_TIMESTAMP_END\", \n",
    "                             \"STATUS\", \n",
    "                             \"DEVICE_CATEGORY\",\n",
    "                             \"RUNNING_VARIABLE\", \"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                pd_agg_cp_clickstream_dp = pd_cp_clickstream_dp.groupby(groupVars)[[\"CP\",\"CP_V2\",\"CP_V3\",\"CP_V4\"]].sum().reset_index()\n",
    "\n",
    "                if row == 0:\n",
    "\n",
    "                    pd_clickstream_cp_outcome =  pd_agg_cp_clickstream_dp\n",
    "\n",
    "                else:\n",
    "\n",
    "                    pd_clickstream_cp_outcome = pd_clickstream_cp_outcome.append(pd_agg_cp_clickstream_dp)\n",
    "\n",
    "                # print(\"==========GENERATION IS COMPLETED FOR DATA POINT: \" + str(row) + \" ==========\")\n",
    "                # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "            # print(\"--- TOTAL --- %s seconds ---\" % (time.time() - start_time_0))\n",
    "\n",
    "\n",
    "            clickstream_rdd_sample_cleaned[\"CUSTOMER_ID\"] = clickstream_rdd_sample_cleaned[\"CUSTOMER_ID\"].astype(str)\n",
    "            pd_clickstream_cp_outcome[\"CUSTOMER_ID\"] = pd_clickstream_cp_outcome[\"CUSTOMER_ID\"].astype(str)\n",
    "\n",
    "            rdd_sample = pd.merge(clickstream_rdd_sample_cleaned, pd_clickstream_cp_outcome, on = groupVars, how = \"left\")\n",
    "\n",
    "            second_layer_index = str(cutoff_point) + \"_\" + str(cutoff_point_name) + \"_\" + str(window_length)\n",
    "\n",
    "            print(\"CP NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V2 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V2\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V3 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V3\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V4 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V4\"].isna())/len(rdd_sample)))\n",
    "\n",
    "            rdd_sample = rdd_sample.fillna({\"CP\":0,\n",
    "                                           \"CP_V2\":0,\n",
    "                                            \"CP_V3\":0,\n",
    "                                            \"CP_V4\":0})\n",
    "\n",
    "            clickstream_rdd_sample_collection_temp[second_layer_index] = rdd_sample\n",
    "        \n",
    "        \n",
    "    print(\"--- TOTAL RUNTIME FOR RDD CONSTRUCTION OF\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "        \n",
    "    return clickstream_rdd_sample_collection_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "212d1e80-e65c-4457-9e5a-a2f64571401e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11. Add kernel weights to RDD regression\n",
    "\n",
    "def kernel(R, h, a1, a2):\n",
    "    indicator = (np.abs(R) <= h).astype(float)\n",
    "    return (indicator * (1 - np.abs(R)/h) / a1) ** a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c6da0c-8832-4da9-b2b3-8996400f849c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 12. Clickstream RDD\n",
    "def clickstream_rdd_modeling(clickstream_rdd_sample_collection, first_layer_index, cp_variable, K, regression_type, h, a1, a2, bootstrapping_method, boostramp_sample_size):\n",
    "\n",
    "    clickstream_rdd_estimate_collection = {}\n",
    "    \n",
    "    print(\"========== RUNNING CLICKSTREAM RDD FOR \" + first_layer_index + \" ==========\")\n",
    "    \n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    if regression_type == \"SEPARATE_UNWEIGHTED\":\n",
    "\n",
    "        for sample_index, pd_rdd_sample in clickstream_rdd_sample_collection.items():\n",
    "            print(\"========== NOW RUNNING SEPARATE_UNWEIGHTED REGRESSION FOR SAMPLE: \" + str(sample_index) + \"==========\")\n",
    "\n",
    "            print(\"DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "            pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "            pd_rdd_sample\n",
    "\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "                \n",
    "                plt.figure(0)\n",
    "\n",
    "                x1 = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "                plt.figure(1)\n",
    "            \n",
    "                # another version of the graph\n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "\n",
    "                \n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= 0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()        \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show() \n",
    "\n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "\n",
    "                results = sm.OLS(Y, X).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(CLICKSTREAM) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "\n",
    "                estimate_list = []\n",
    "                for s in range(boostramp_sample_size):\n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "                    \n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.OLS(Y, X).fit()\n",
    "                    \n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                    \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "            clickstream_rdd_estimate_collection[sample_index] = {}\n",
    "            clickstream_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "            clickstream_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\n",
    "\n",
    "    elif regression_type == \"SEPARATE_WEIGHTED\":\n",
    "\n",
    "        for sample_index, pd_rdd_sample in clickstream_rdd_sample_collection.items():\n",
    "            print(\"========== NOW RUNNING SEPARATE_WEIGHTED REGRESSION FOR SAMPLE: \" + str(sample_index) + \"==========\")\n",
    "\n",
    "            print(\"DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "            pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "            pd_rdd_sample\n",
    "\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "\n",
    "                \n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()      \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                \n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show() \n",
    "\n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(CLICKSTREAM) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                estimate_list = []\n",
    "                \n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "            clickstream_rdd_estimate_collection[sample_index] = {}\n",
    "            clickstream_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "            clickstream_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\t\t\n",
    "    elif regression_type == \"JOINT_WEIGHTED\":\n",
    "\n",
    "        index = 1\n",
    "        \n",
    "        for sample_index_temp, pd_rdd_sample_temp in clickstream_rdd_sample_collection.items():\n",
    "            \n",
    "            print(\"========== NOW RUNNING JOINT_WEIGHTED REGRESSION ==========\")\n",
    "            print(\"FOR SAMPLE \" + sample_index_temp + \", DATA POINTS: \" + str(len(pd_rdd_sample_temp)))\n",
    "\n",
    "            pd_rdd_sample_temp[\"SAMPLE_INDEX\"] = index\n",
    "\n",
    "            if index == 1:\n",
    "\n",
    "                pd_rdd_sample = pd_rdd_sample_temp\n",
    "                sample_index = sample_index_temp\n",
    "\n",
    "            else:\n",
    "\n",
    "                pd_rdd_sample = pd_rdd_sample.append(pd_rdd_sample_temp)\n",
    "                sample_index = sample_index_temp\n",
    "\n",
    "            index = index + 1\n",
    "            \n",
    "            print(\"JOINT SAMPLE HAS DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "\n",
    "        print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "        pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "        pd_rdd_sample\n",
    "\n",
    "        print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "        print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "\n",
    "\n",
    "        if np.max(pd_rdd_sample[\"SAMPLE_INDEX\"]) == 1:\n",
    "            \n",
    "            print(\"NO NEED TO JOIN AND PROCEED AS SEPARATE REGRESSION\")\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\") \n",
    "            \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] >= -h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"RUNNING_VARIABLE_1\"] <= h) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "                # another version of the graph\n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")   \n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()        \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(CLICKSTREAM) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "\n",
    "                estimate_list = []\n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            index_var_name_list = []\n",
    "            \n",
    "            for m in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1):\n",
    "                \n",
    "                index_var_name = \"SAMPLE_INDEX\" + \"_\" + str(m)\n",
    "                pd_rdd_sample[index_var_name] = np.where(pd_rdd_sample[\"SAMPLE_INDEX\"] == m, 1, 0)\n",
    "                index_var_name_list = index_var_name_list + [index_var_name]\n",
    "\n",
    "            vars_for_interaction_with_sample_index = [\"RUNNING_VARIABLE_1\", \"RUNNING_VARIABLE_2\", \"RUNNING_VARIABLE_3\", \"RUNNING_VARIABLE_4\",\n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_1\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_2\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_3\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_4\"]\n",
    "            \n",
    "            interaction_var_name_list = []\n",
    "            \n",
    "            for m1 in index_var_name_list:\n",
    "                for m2 in vars_for_interaction_with_sample_index:\n",
    "                    \n",
    "                    interaction_var_name = m2 + \"_\" + m1\n",
    "                    pd_rdd_sample[interaction_var_name] = pd_rdd_sample[m1] * pd_rdd_sample[m2]\n",
    "                    interaction_var_name_list = interaction_var_name_list + [interaction_var_name]\n",
    "\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] \\\n",
    "            + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] \\\n",
    "            + [\"RUNNING_VARIABLE_\" + str(n1)  + \"_\" + \"SAMPLE_INDEX_\" + str(n2) for n1 in np.arange(1,K+1) for n2 in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1)] \\\n",
    "            + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n1)  + \"_\" + \"SAMPLE_INDEX_\" + str(n2) for n1 in np.arange(1,K+1) for n2 in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(CLICKSTREAM) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                estimate_list = []\n",
    "                \n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                    \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(CLICKSTREAM) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "                \n",
    "        clickstream_rdd_estimate_collection[sample_index] = {}\n",
    "        clickstream_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "        clickstream_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\n",
    "    print(\"--- TOTAL RUNTIME OF CLICKSTREAM RDD FOR\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "        \n",
    "    return clickstream_rdd_estimate_collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad233f9b-47cb-4f09-aa80-f8ea519c6841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 13. Clickstream RDD result summaries\n",
    "\n",
    "\n",
    "def clickstream_rdd_result_summary(clickstream_rdd_estimate_collection, first_layer_index):\n",
    "    \n",
    "    print(\"========== RUNNING CLICKSTREAM RDD FOR \" + first_layer_index + \" ==========\")\n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    clickstream_rdd_estimate_value_list = []\n",
    "    clickstream_rdd_estimate_std_error_list = []\n",
    "\n",
    "    for result_index, result_sample in clickstream_rdd_estimate_collection.items():\n",
    "\n",
    "        clickstream_rdd_estimate_value_list += [result_sample[\"Estimate\"]]\n",
    "        clickstream_rdd_estimate_std_error_list += [result_sample[\"StdError\"]]\n",
    "\n",
    "    if len(clickstream_rdd_estimate_value_list) == 1:\n",
    "        final_clickstream_estimate_value = clickstream_rdd_estimate_value_list[0]\n",
    "        final_clickstream_estimate_std_error = clickstream_rdd_estimate_std_error_list[0]\n",
    "        final_clickstream_z_score = final_clickstream_estimate_value/final_clickstream_estimate_std_error\n",
    "        final_clickstream_statistical_significance_p05 = np.abs(final_clickstream_z_score) >= 1.96\n",
    "        final_clickstream_statistical_significance_p10 = np.abs(final_clickstream_z_score) >= 1.645\n",
    "        print(\"(CLICKSTREAM) FINAL RDD ESTIMATE FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_estimate_value))\n",
    "        print(\"(CLICKSTREAM) FINAL RDD ESTIMATE STD ERROR FOR \" + first_layer_index + \" IS: \"+ str(final_clickstream_estimate_std_error))\n",
    "        print(\"(CLICKSTREAM) FINAL Z SCORE FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_z_score))\n",
    "        print(\"(CLICKSTREAM) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 5% FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_statistical_significance_p05))\n",
    "        print(\"(CLICKSTREAM) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 10% FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_statistical_significance_p10))\n",
    "\n",
    "    else:\n",
    "\n",
    "        np_clickstream_rdd_estimate_values = np.array(clickstream_rdd_estimate_value_list)\n",
    "        np_clickstream_rdd_estimate_values_2 = np_clickstream_rdd_estimate_values**2\n",
    "        np_clickstream_rdd_estimate_std_errors = np.array(clickstream_rdd_estimate_std_error_list)\n",
    "        np_clickstream_rdd_estimate_variance = np_clickstream_rdd_estimate_std_errors**2\n",
    "\n",
    "        # assign equal weights to each estimate\n",
    "        clickstream_np_weights = np.array([1/len(clickstream_rdd_estimate_value_list)] * len(clickstream_rdd_estimate_value_list))\n",
    "\n",
    "        # final_estimate_value = np.sum(np.array(rdd_estimate_value_list)*np_optimal_weights)\n",
    "        final_clickstream_estimate_value = np.sum(np_clickstream_rdd_estimate_values * clickstream_np_weights) \n",
    "        final_clickstream_estimate_variance =  np.sum(np_clickstream_rdd_estimate_variance * clickstream_np_weights)\\\n",
    "        + np.sum(np_clickstream_rdd_estimate_values_2 * clickstream_np_weights)\\\n",
    "        - np.square(final_clickstream_estimate_value)\n",
    "        final_clickstream_estimate_std_error = np.sqrt(final_clickstream_estimate_variance)\n",
    "\n",
    "        final_clickstream_z_score = final_clickstream_estimate_value / final_clickstream_estimate_std_error\n",
    "        final_clickstream_statistical_significance_p05 = np.abs(final_clickstream_z_score) >= 1.96\n",
    "        final_clickstream_statistical_significance_p10 = np.abs(final_clickstream_z_score) >= 1.645\n",
    "        print(\"(CLICKSTREAM) FINAL RDD ESTIMATE FOR: \" + str(final_clickstream_estimate_value))\n",
    "        print(\"(CLICKSTREAM) FINAL RDD ESTIMATE STD ERROR FOR \" + str(final_clickstream_estimate_std_error))\n",
    "        print(\"(CLICKSTREAM) FINAL Z SCORE FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_z_score))\n",
    "        print(\"(CLICKSTREAM) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 5% FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_statistical_significance_p05))\n",
    "        print(\"(CLICKSTREAM) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 10% FOR \" + first_layer_index + \" IS: \" + str(final_clickstream_statistical_significance_p10))\n",
    "        \n",
    "    finalized_clickstream_rdd_estimates = {}\n",
    "    finalized_clickstream_rdd_estimates[\"Estimate\"] = final_clickstream_estimate_value\n",
    "    finalized_clickstream_rdd_estimates[\"StdError\"] = final_clickstream_estimate_std_error\n",
    "    finalized_clickstream_rdd_estimates[\"ZScore\"] = final_clickstream_z_score\n",
    "    finalized_clickstream_rdd_estimates[\"Significance_5_perc\"] = final_clickstream_statistical_significance_p05\n",
    "    finalized_clickstream_rdd_estimates[\"Significance_10_perc\"] = final_clickstream_statistical_significance_p10 \n",
    "    \n",
    "    \n",
    "    print(\"--- TOTAL RUNTIME OF CLICKSTREAM RDD RESULT SUMMARY FOR\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "    \n",
    "    return finalized_clickstream_rdd_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3fc8ba3-ccdf-47df-8cca-eb30985b5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Generate pd_valid_clickstream_rdd_estimates\n",
    "def generate_pd_valid_clickstream_rdd_estimates(finalized_clickstream_rdd_estimates):\n",
    "    \n",
    "    k = 0\n",
    "\n",
    "    for key, value in finalized_clickstream_rdd_estimates.items():\n",
    "\n",
    "        key_list = key.split(\"_\")\n",
    "        Product = key_list[0]\n",
    "        Year = key_list[1]\n",
    "        Month = key_list[2]\n",
    "\n",
    "        Estimate = value[\"Estimate\"]\n",
    "        StdError = value[\"StdError\"]\n",
    "        ZScore = value[\"ZScore\"]\n",
    "        Significance_5_perc = value[\"Significance_5_perc\"]\n",
    "        Significance_10_perc = value[\"Significance_10_perc\"]\n",
    "\n",
    "        if Significance_10_perc == True:\n",
    "            if k == 0:\n",
    "                valid_rdd_estimates_data = [[Product, Year, Month, Estimate, StdError]]\n",
    "            else:\n",
    "                valid_rdd_estimates_data = valid_rdd_estimates_data +  [[Product, Year, Month, Estimate, StdError]]\n",
    "\n",
    "            k = k + 1\n",
    "\n",
    "    pd_valid_rdd_estimates = pd.DataFrame(valid_rdd_estimates_data, columns = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\", \"ESTIMATE\", \"STD_ERROR\"])\n",
    "    pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"] = pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "    return pd_valid_rdd_estimates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e129f70-8459-48bd-8ac1-85fd1f5b7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Generate pd_valid_clickstream_rdd_estimates\n",
    "def generate_pd_valid_clickstream_rdd_estimates(finalized_clickstream_rdd_estimates):\n",
    "    \n",
    "    k = 0\n",
    "\n",
    "    for key, value in finalized_clickstream_rdd_estimates.items():\n",
    "\n",
    "        key_list = key.split(\"_\")\n",
    "        Product = key_list[0]\n",
    "        Year = key_list[1]\n",
    "        Month = key_list[2]\n",
    "\n",
    "        Estimate = value[\"Estimate\"]\n",
    "        StdError = value[\"StdError\"]\n",
    "        ZScore = value[\"ZScore\"]\n",
    "        Significance_5_perc = value[\"Significance_5_perc\"]\n",
    "        Significance_10_perc = value[\"Significance_10_perc\"]\n",
    "\n",
    "        if Significance_10_perc == True:\n",
    "            if k == 0:\n",
    "                valid_rdd_estimates_data = [[Product, Year, Month, Estimate, StdError]]\n",
    "            else:\n",
    "                valid_rdd_estimates_data = valid_rdd_estimates_data +  [[Product, Year, Month, Estimate, StdError]]\n",
    "\n",
    "            k = k + 1\n",
    "\n",
    "    pd_valid_rdd_estimates = pd.DataFrame(valid_rdd_estimates_data, columns = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\", \"ESTIMATE\", \"STD_ERROR\"])\n",
    "    pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"] = pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "    return pd_valid_rdd_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c11c58d-ccb1-46af-bb35-1ab5c158bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Generate time values & product x time values\n",
    "def generate_clickstream_productxtime_sets(year_list, month_list, product_list, pd_sku_features):\n",
    "    \n",
    "    # 3.2.1. time values \n",
    "    time_values = [[m1, m2] for m1 in year_list for m2 in month_list]\n",
    "    pd_time_values = pd.DataFrame(time_values, columns = [\"YEAR\", \"MONTH\"])\n",
    "    pd_time_values[\"YEAR\"] = pd_time_values[\"YEAR\"].astype(str)\n",
    "    pd_time_values[\"MONTH\"] = pd_time_values[\"MONTH\"].astype(str)\n",
    "\n",
    "    # 3.2.2. product-time values \n",
    "    product_x_time_values = [[m1,m2,m3] for m1 in product_list for m2 in year_list for m3 in month_list]\n",
    "    pd_product_x_time_values = pd.DataFrame(product_x_time_values, columns = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\"])\n",
    "    pd_product_x_time_values[\"PRODUCT_PART_NUMBER\"] = pd_product_x_time_values[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "    pd_product_x_time_values[\"YEAR\"] = pd_product_x_time_values[\"YEAR\"].astype(str)\n",
    "    pd_product_x_time_values[\"MONTH\"] = pd_product_x_time_values[\"MONTH\"].astype(str)\n",
    "    \n",
    "    # 3.2.3. Add features\n",
    "    pd_product_x_time_with_sku_features = pd_product_x_time_values.merge(pd_sku_features, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\n",
    "    \n",
    "    return pd_time_values, pd_product_x_time_with_sku_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "710cfc4c-4755-41dc-a913-ddb2f5c8ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Remediation: Generate final remediated estimates\n",
    "\n",
    "def generate_clickstream_remediated_estimates(year_list, month_list, pd_time_values, pd_valid_rdd_estimates_with_sku_features, pd_product_x_time_with_sku_features):\n",
    "\n",
    "    k=0\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "\n",
    "            # Get the slice that spans month 0 to the current month \n",
    "            pd_time_values_month0_to_current_month = pd_time_values.iloc[0:(k+1),:]\n",
    "            pd_valid_rdd_estimates_with_sku_features_seed = pd_valid_rdd_estimates_with_sku_features.merge(pd_time_values_month0_to_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Get the slice that covers the current month (for constructing the complete results for the current month)\n",
    "            pd_time_values_current_month = pd_time_values.iloc[k:(k+1),:]\n",
    "            pd_valid_rdd_estimates_with_sku_features_current_month = pd_valid_rdd_estimates_with_sku_features.merge(pd_time_values_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Get the slice that covers the current month (for constructing the complete results for the current month  serving as left side)\n",
    "            pd_product_x_time_with_sku_features_current_month =  pd_product_x_time_with_sku_features.merge(pd_time_values_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Summarize by SKU\n",
    "            avg_estimate_by_SKU = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"PRODUCT_PART_NUMBER\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_SKU.columns = [\"PRODUCT_PART_NUMBER\", \"SKU_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1-MC2-MC3 combination\n",
    "            avg_estimate_by_MC1_MC2_MC3 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\",\"MC2\",\"MC3\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1_MC2_MC3.columns = [\"MC1\",\"MC2\",\"MC3\", \"MC1_MC2_MC3_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1-MC2 combination\n",
    "            avg_estimate_by_MC1_MC2 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\", \"MC2\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1_MC2.columns = [\"MC1\", \"MC2\", \"MC1_MC2_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1\n",
    "            avg_estimate_by_MC1 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1.columns = [\"MC1\", \"MC1_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Overall estimate\n",
    "            avg_estimate_overall = np.mean(pd_valid_rdd_estimates_with_sku_features_seed[\"ESTIMATE\"])\n",
    "\n",
    "            # Now we merge the average results with current month results to complete remediation \n",
    "            matching_vars = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\", \"MC1\", \"MC2\", \"MC3\"]\n",
    "            pd_product_x_time_with_sku_features_current_month_remediated = pd_product_x_time_with_sku_features_current_month.merge(pd_valid_rdd_estimates_with_sku_features_current_month, on = matching_vars, how = \"left\")\\\n",
    "            .merge(avg_estimate_by_SKU, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1_MC2_MC3, on = [\"MC1\", \"MC2\", \"MC3\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1_MC2, on = [\"MC1\", \"MC2\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1, on = [\"MC1\"], how = \"left\")\n",
    "            pd_product_x_time_with_sku_features_current_month_remediated[\"AVERAGE_ESTIMATE_OVERALL\"] = avg_estimate_overall\n",
    "\n",
    "            # Fill in missing values in the order of organic estimate, SKU average, MC1-MC2-MC3 average, MC1-MC2 average, M1 average, overall average \n",
    "            pd_product_x_time_with_sku_features_current_month_remediated[\"REMEDIATED_ESTIMATE\"] = np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"ESTIMATE\"],\n",
    "                                                                                                          np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"SKU_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"SKU_AVG_ESTIMATE\"],\n",
    "                                                                                                                  np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_MC3_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_MC3_AVG_ESTIMATE\"],\n",
    "                                                                                                                          np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_AVG_ESTIMATE\"], \n",
    "                                                                                                                                  np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_AVG_ESTIMATE\"], pd_product_x_time_with_sku_features_current_month_remediated[\"AVERAGE_ESTIMATE_OVERALL\"])))))\n",
    "\n",
    "\n",
    "            # Construct the complete estimates from month 0 to current month\n",
    "            if k == 0:\n",
    "                pd_remediated_estimates = pd_product_x_time_with_sku_features_current_month_remediated\t\t\n",
    "            else:\n",
    "                pd_remediated_estimates = pd_remediated_estimates.append(pd_product_x_time_with_sku_features_current_month_remediated)\n",
    "\n",
    "\n",
    "            k=k+1\n",
    "            \n",
    "    pd_remediated_estimates = pd_remediated_estimates.reset_index()             \n",
    "    \n",
    "    return pd_remediated_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f29f5e61-bb9d-49f3-95fb-5581a4fb8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. AS orders slice\n",
    "def as_orders_slice_pandas(df_as_orders_data, sample_sku, sample_year, sample_month):\n",
    "    \n",
    "    df_as_orders_data_slice = df_as_orders_data.where((F.col(\"PRODUCT_PART_NUMBER\")== str(sample_sku))\\\n",
    "                                                & (F.col(\"ORDER_PLACED_YEAR\") == sample_year) \\\n",
    "                                                & (F.col(\"ORDER_PLACED_MONTH\") == sample_month))\n",
    "  \n",
    "    pd_as_orders_data_slice = df_as_orders_data_slice.toPandas()\n",
    "    \n",
    "    return df_as_orders_data_slice, pd_as_orders_data_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1605458-dbbb-4bdc-ba77-461bf048a760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 19. AS orders count summaries\n",
    "\n",
    "def as_orders_by_instock_status(pd_as_orders_data_slice):\n",
    "    \n",
    "    as_order_identifier = [\"CUSTOMER_ID\", \n",
    "                       \"ORDER_ID\",\n",
    "                       \"ORDER_LINE_ID\",\n",
    "                       \"ORDER_PLACED_DTTM\",\n",
    "                        \"PRODUCT_PART_NUMBER\", \n",
    "                        \"ORDER_ITEM_BACKORDER_FLAG\", \n",
    "                        \"ORDER_PLACED_YEAR\", \"ORDER_PLACED_MONTH\"]\n",
    "\n",
    "    is_as_orders_counts = len(pd_as_orders_data_slice[pd_as_orders_data_slice[\"ORDER_ITEM_BACKORDER_FLAG\"] == False][as_order_identifier].drop_duplicates())\n",
    "    oos_as_orders_counts = len(pd_as_orders_data_slice[pd_as_orders_data_slice[\"ORDER_ITEM_BACKORDER_FLAG\"] == True][as_order_identifier].drop_duplicates())\n",
    "    total_as_orders_counts = len(pd_as_orders_data_slice[as_order_identifier].drop_duplicates())    \n",
    "\n",
    "    return is_as_orders_counts, oos_as_orders_counts, total_as_orders_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d9e4ba-0830-41b1-b99f-e6b6e303ad14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 20. Find as orders cutoff dates\n",
    "\n",
    "def find_as_orders_cutoff_dates(pd_as_orders_data_slice):\n",
    "\n",
    "    # 1. Dummy variables that indicate IS and OOS\n",
    "    \n",
    "    pd_as_orders_data_slice[\"IS\"] = np.where(pd_as_orders_data_slice[\"ORDER_ITEM_BACKORDER_FLAG\"] == False, 1, 0)\n",
    "    pd_as_orders_data_slice[\"OOS\"] = np.where(pd_as_orders_data_slice[\"ORDER_ITEM_BACKORDER_FLAG\"] == True, 1, 0)\n",
    "    \n",
    "    pd_as_orders_is_oos_counts_by_date = pd_as_orders_data_slice.groupby([\"ORDER_PLACED_DATE\"])[[\"IS\", \"OOS\"]].sum().reset_index()\n",
    "    pd_as_orders_is_oos_counts_by_date[\"IS_ADJ\"] = np.where(pd_as_orders_is_oos_counts_by_date[\"IS\"]/(pd_as_orders_is_oos_counts_by_date[\"IS\"]+pd_as_orders_is_oos_counts_by_date[\"OOS\"])<0.1, 0, pd_as_orders_is_oos_counts_by_date[\"IS\"]) \n",
    "    pd_as_orders_is_oos_counts_by_date[\"OOS_ADJ\"] = np.where(pd_as_orders_is_oos_counts_by_date[\"OOS\"]/(pd_as_orders_is_oos_counts_by_date[\"IS\"]+pd_as_orders_is_oos_counts_by_date[\"OOS\"])<0.1, 0, pd_as_orders_is_oos_counts_by_date[\"OOS\"]) \n",
    "    \n",
    "    # 2. Generate status switching variable\n",
    "\n",
    "    N = len(pd_as_orders_is_oos_counts_by_date)\n",
    "\n",
    "    for row in range(N):\n",
    "\n",
    "        if row == 0:\n",
    "\n",
    "            temp = pd_as_orders_is_oos_counts_by_date.iloc[row:(row+1),].reset_index()\n",
    "            is_counts = temp[\"IS_ADJ\"][0]\n",
    "            oos_counts = temp[\"OOS_ADJ\"][0]\n",
    "            \n",
    "            # initialization for the first date in this monthly sample \n",
    "            if (is_counts == 0) & (oos_counts == 0):\n",
    "                temp_status = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0):\n",
    "                temp_status = \"IS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0):\n",
    "                temp_status = \"OOS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0):\n",
    "                temp_status = \"SWITCHING TO IS\"\n",
    "\n",
    "            temp[\"SWITCHING_STATUS\"] = temp_status\n",
    "\n",
    "            pd_as_orders_cutoff_point_detection = temp\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp_t_minus_1 = pd_as_orders_cutoff_point_detection.iloc[(row-1):row,].reset_index()\n",
    "            switching_status_t_minus_1 = temp_t_minus_1[\"SWITCHING_STATUS\"][0]\n",
    "\n",
    "            temp_t = pd_as_orders_is_oos_counts_by_date.iloc[row:(row+1),].reset_index()\n",
    "            is_counts = temp_t[\"IS_ADJ\"][0]\n",
    "            oos_counts = temp_t[\"OOS_ADJ\"][0]\n",
    "            \n",
    "            if (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "            elif (is_counts > 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"    \n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "            elif (is_counts > 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS\"\n",
    "\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"OOS\"    \n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"OOS\"\n",
    "            elif (is_counts == 0) & (oos_counts > 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"OOS\"\n",
    "\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"IS\"):\n",
    "                switching_status_t = \"IS\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"OOS\"):\n",
    "                switching_status_t = \"OOS\"    \n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO IS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO IS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "            elif (is_counts == 0) & (oos_counts == 0) & (switching_status_t_minus_1 == \"SWITCHING TO OOS (NON-CUTOFF)\"):\n",
    "                switching_status_t = \"SWITCHING TO OOS (NON-CUTOFF)\"\n",
    "\n",
    "            else:\n",
    "                switching_status_t = np.nan\n",
    "                print(\"There is NULL value in the table.\")\n",
    "\n",
    "            temp_t[\"SWITCHING_STATUS\"] = switching_status_t\n",
    "            \n",
    "            pd_as_orders_cutoff_point_detection = pd_as_orders_cutoff_point_detection.append(temp_t)\n",
    "\n",
    "    pd_as_orders_cutoff_points = pd_as_orders_cutoff_point_detection[pd_as_orders_cutoff_point_detection[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"])][[\"ORDER_PLACED_DATE\",\"SWITCHING_STATUS\"]]    \n",
    "    \n",
    "    return pd_as_orders_cutoff_point_detection, pd_as_orders_cutoff_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab23f527-e5ca-425b-85ce-329b6e6a7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Scheduled AS orders RDD customers\n",
    "\n",
    "def find_as_orders_rdd_relevant_customers(pd_as_orders_cutoff_points, pd_as_orders_cutoff_point_detection, pd_as_orders_data_slice, sufficiency_ratio, initial_window_length, first_layer_index):\n",
    "     \n",
    "    # print(\"==========CONSTRUCTING AS_ORDERS RDD CUSTOMER SAMPLE FOR \" + first_layer_index + \" ==========\")\n",
    "\n",
    "    # as_orders_rdd_customers_temp = pd.DataFrame(columns = [\"CUSTOMER_ID\"])\n",
    "\n",
    "    as_orders_rdd_customers_temp = []\n",
    "    \n",
    "    # 1. RDD sample generation\n",
    "    for k in range(len(pd_as_orders_cutoff_points)):\n",
    "\n",
    "        cutoff_point = pd_as_orders_cutoff_points.iloc[k,][\"ORDER_PLACED_DATE\"]\n",
    "\n",
    "        cutoff_point_name = pd_as_orders_cutoff_points.iloc[k,][\"SWITCHING_STATUS\"]\n",
    "\n",
    "        window_length = initial_window_length\n",
    "\n",
    "        # Check T days prior to the cutoff date and T days after the cutoff date\n",
    "        while window_length > 0:\n",
    "\n",
    "            # Window start date\n",
    "            window_left_end = max(cutoff_point - DT.timedelta(days=window_length), min(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"]))\n",
    "\n",
    "            # Window end date\n",
    "            window_right_end = min(cutoff_point + DT.timedelta(days=(window_length+1)), max(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"])+ DT.timedelta(days=1))\n",
    "\n",
    "            pd_as_orders_cutoff_point_detection_rdd_sample_temp = pd_as_orders_cutoff_point_detection[(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"] >= window_left_end) & (pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"] < window_right_end) ]\n",
    "\n",
    "            # [1] We want to make sure this sample only has the cutoff point that is under inspection\n",
    "            if np.sum(pd_as_orders_cutoff_point_detection_rdd_sample_temp[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"]))>1:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:\n",
    "                    # print(\"THIS IS THE SHORTEST AS ORDERS RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT IS STILL NOT CLEAN\")\n",
    "                    temp = 1\n",
    "                continue\n",
    "            else:\n",
    "                # print(\"FOUND A CLEAN AS ORDERS RDD SAMPLE FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"CHECKING DATA SUFFICIENCY\")\n",
    "                temp = 1\n",
    "\n",
    "            # [1] AS ORDERS RDD horizon\n",
    "            as_orders_rdd_sample_temp = pd_as_orders_data_slice[ (pd_as_orders_data_slice[\"ORDER_PLACED_DATE\"]>=window_left_end) & (pd_as_orders_data_slice[\"ORDER_PLACED_DATE\"]< window_right_end)]\n",
    "\n",
    "            # [2] Remove duplicate clickstream data points   \n",
    "            as_orders_identifier_variables = [\"CUSTOMER_ID\", \n",
    "                           \"ORDER_ID\",\n",
    "                           \"ORDER_LINE_ID\",\n",
    "                           \"ORDER_PLACED_DTTM\",\n",
    "                            \"PRODUCT_PART_NUMBER\", \n",
    "                            \"ORDER_ITEM_BACKORDER_FLAG\", \n",
    "                            \"ORDER_STATUS\",\n",
    "                            \"ORDER_PLACED_DATE\", \"ORDER_PLACED_YEAR\", \"ORDER_PLACED_MONTH\"]\n",
    "            as_orders_rdd_sample_cleaned = as_orders_rdd_sample_temp[as_orders_identifier_variables].drop_duplicates()\n",
    "\n",
    "            # [3] Ensure at least 50% of days on both sides of the cutoff point need to have data \n",
    "            # Summarize daily data points\n",
    "            as_orders_counts_distribution_across_days = as_orders_rdd_sample_cleaned.groupby([\"ORDER_PLACED_DATE\"])[\"CUSTOMER_ID\"].count().reset_index()\n",
    "\n",
    "            if (as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] < cutoff_point][\"ORDER_PLACED_DATE\"].nunique() >= window_length*sufficiency_ratio) & \\\n",
    "                (as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] > cutoff_point][\"ORDER_PLACED_DATE\"].nunique() >= window_length*sufficiency_ratio):\n",
    "                # print(\"WE FOUND SUFFICIENT DATA FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"LEFT SIDE HAS \" + str(as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] < cutoff_point][\"ORDER_PLACED_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"RIGHT SIDE HAS \" + str(as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] > cutoff_point][\"ORDER_PLACED_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"THIS RDD SAMPLE IS COLLECTED FOR REGRESSION AND ANALYSES\")\n",
    "                temp = 1\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:   \n",
    "                    # print(\"THIS IS THE SHORTEST AS ORDERS RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT DOES NOT HAVE SUFFICIENT DATA\")\n",
    "                    temp = 1\n",
    "                continue\n",
    "\n",
    "        if window_length == 0:\n",
    "            # print(\"UNFORTUNATELY, WE CANNOT FIND AN IDEAL AS ORDERS RDD SAMPLE FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            temp = 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # print(\"WE HAVE FOUND THE RDD SAMPLE FOR REGRESSION FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            # print(\"THE LEFT END DATE IS: \" + str(window_left_end))\n",
    "            # print(\"THE RIGHT END DATE IS: \" + str(window_right_end))\n",
    "            temp = 1\n",
    "\n",
    "            # [1] Collect the customers\n",
    "\n",
    "            as_orders_rdd_customers_temp0 = as_orders_rdd_sample_cleaned[[\"CUSTOMER_ID\"]].drop_duplicates()[\"CUSTOMER_ID\"].to_list()\n",
    "\n",
    "            second_layer_index = str(cutoff_point) + \"_\" + str(cutoff_point_name) + \"_\" + str(window_length)\n",
    "\n",
    "            # print(\"(AS_ORDERS) THE NUMBER OF CUSTOMERS IN \" + first_layer_index + \" SAMPLE \" + second_layer_index + \" IS \" + str(len(as_orders_rdd_customers_temp0)))\n",
    "\n",
    "            # as_orders_rdd_customers_temp = as_orders_rdd_customers_temp.append(as_orders_rdd_customers_temp0).drop_duplicates().reset_index()[[\"CUSTOMER_ID\"]]\n",
    "    \n",
    "            as_orders_rdd_customers_temp = as_orders_rdd_customers_temp + as_orders_rdd_customers_temp0\n",
    "            as_orders_rdd_customers_temp = [*set(as_orders_rdd_customers_temp)]\n",
    "        \n",
    "    return as_orders_rdd_customers_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8406a8-0e3c-41bb-bcda-539691a56644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 22. AS Orders RDD sample construction\n",
    "def construct_as_orders_rdd_samples(pd_as_orders_cutoff_points, pd_as_orders_cutoff_point_detection, pd_as_orders_data_slice, sufficiency_ratio, initial_window_length, first_layer_index, df_cp):\n",
    "     \n",
    "    print(\"==========CONSTRUCTING AS ORDERS RDD SAMPLE FOR \" + first_layer_index + \" ==========\")\n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    as_orders_rdd_sample_collection_temp = {}\n",
    "    \n",
    "    # 1. RDD sample generation\n",
    "    for k in range(len(pd_as_orders_cutoff_points)):\n",
    "\n",
    "        cutoff_point = pd_as_orders_cutoff_points.iloc[k,][\"ORDER_PLACED_DATE\"]\n",
    "\n",
    "        cutoff_point_name = pd_as_orders_cutoff_points.iloc[k,][\"SWITCHING_STATUS\"]\n",
    "\n",
    "        window_length = initial_window_length\n",
    "\t\n",
    "        # Check T days prior to the cutoff date and T days after the cutoff date\n",
    "        while window_length > 0:\n",
    "\n",
    "            # Window start date\n",
    "            window_left_end = max(cutoff_point - DT.timedelta(days=window_length), min(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"]))\n",
    "\n",
    "            # Window end date\n",
    "            window_right_end = min(cutoff_point + DT.timedelta(days=(window_length+1)), max(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"])+ DT.timedelta(days=1))\n",
    "\n",
    "            pd_as_orders_cutoff_point_detection_rdd_sample_temp = pd_as_orders_cutoff_point_detection[(pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"] >= window_left_end) & (pd_as_orders_cutoff_point_detection[\"ORDER_PLACED_DATE\"] < window_right_end) ]\n",
    "\n",
    "            # [1] We want to make sure this sample only has the cutoff point that is under inspection\n",
    "            if np.sum(pd_as_orders_cutoff_point_detection_rdd_sample_temp[\"SWITCHING_STATUS\"].isin([\"SWITCHING TO IS\", \"SWITCHING TO OOS\"]))>1:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:\n",
    "                    # print(\"THIS IS THE SHORTEST AS ORDERS RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT IS STILL NOT CLEAN\")\n",
    "                    temp = 1\n",
    "                continue\n",
    "            else:\n",
    "                # print(\"FOUND A CLEAN AS ORDERS RDD SAMPLE FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"CHECKING DATA SUFFICIENCY\")\n",
    "                temp = 1\n",
    "                \n",
    "            # [1] AS ORDERS RDD horizon\n",
    "            as_orders_rdd_sample_temp = pd_as_orders_data_slice[ (pd_as_orders_data_slice[\"ORDER_PLACED_DATE\"]>=window_left_end) & (pd_as_orders_data_slice[\"ORDER_PLACED_DATE\"]< window_right_end)]\n",
    "\n",
    "            # [2] Remove duplicate clickstream data points   \n",
    "            as_orders_identifier_variables = [\"CUSTOMER_ID\", \n",
    "                           \"ORDER_ID\",\n",
    "                           \"ORDER_LINE_ID\",\n",
    "                           \"ORDER_PLACED_DTTM\",\n",
    "                            \"PRODUCT_PART_NUMBER\", \n",
    "                            \"ORDER_ITEM_BACKORDER_FLAG\", \n",
    "                            \"ORDER_STATUS\",\n",
    "                            \"ORDER_PLACED_DATE\", \"ORDER_PLACED_YEAR\", \"ORDER_PLACED_MONTH\"]\n",
    "            as_orders_rdd_sample_cleaned = as_orders_rdd_sample_temp[as_orders_identifier_variables].drop_duplicates()\n",
    "\n",
    "            # [3] Ensure at least 50% of days on both sides of the cutoff point need to have data \n",
    "            # Summarize daily data points\n",
    "            as_orders_counts_distribution_across_days = as_orders_rdd_sample_cleaned.groupby([\"ORDER_PLACED_DATE\"])[\"CUSTOMER_ID\"].count().reset_index()\n",
    "\n",
    "            if (as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] < cutoff_point][\"ORDER_PLACED_DATE\"].nunique() >= window_length*sufficiency_ratio) & \\\n",
    "                (as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] > cutoff_point][\"ORDER_PLACED_DATE\"].nunique() >= window_length*sufficiency_ratio):\n",
    "                # print(\"WE FOUND SUFFICIENT DATA FOR \" + str(cutoff_point) + \" WITH WINDOW LENGTH \" + str(window_length))\n",
    "                # print(\"LEFT SIDE HAS \" + str(as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] < cutoff_point][\"ORDER_PLACED_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"RIGHT SIDE HAS \" + str(as_orders_counts_distribution_across_days[as_orders_counts_distribution_across_days[\"ORDER_PLACED_DATE\"] > cutoff_point][\"ORDER_PLACED_DATE\"].nunique()) + \" DAYS OF DATA\")\n",
    "                # print(\"THIS RDD SAMPLE IS COLLECTED FOR REGRESSION AND ANALYSES\")\n",
    "                temp = 1\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                window_length = window_length - 1\n",
    "                if window_length == 0:   \n",
    "                    # print(\"THIS IS THE SHORTEST AS ORDERS RDD SAMPLE OF \" + str(cutoff_point) + \" YET IT DOES NOT HAVE SUFFICIENT DATA\")\n",
    "                    temp = 1\n",
    "                continue\n",
    "\n",
    "        if window_length == 0:\n",
    "            # print(\"UNFORTUNATELY, WE CANNOT FIND AN IDEAL AS ORDERS RDD SAMPLE FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            temp = 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # print(\"WE HAVE FOUND THE RDD SAMPLE FOR REGRESSION FOR CUTOFF POINT: \" + str(cutoff_point))\n",
    "            # print(\"THE LEFT END DATE IS: \" + str(window_left_end))\n",
    "            # print(\"THE RIGHT END DATE IS: \" + str(window_right_end))\n",
    "            temp = 1\n",
    "            \n",
    "            # 2. Feature Engineering\n",
    "\n",
    "            # [1] The precise timestamp of the cutoff point\n",
    "            \n",
    "            # We make the following update: if the status switches from OOS to IS then we use the first IS timestamp within the cutoff date as the cutoff timestamp. Similar for IS -> OOS.  \n",
    "            \n",
    "            as_orders_rdd_sample_on_cutoff_date = as_orders_rdd_sample_cleaned[as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DATE\"] == cutoff_point]\n",
    "           \n",
    "            as_orders_rdd_sample_on_cutoff_date[\"STATUS_RANK\"] = as_orders_rdd_sample_on_cutoff_date.groupby(\"ORDER_ITEM_BACKORDER_FLAG\")[\"ORDER_PLACED_DTTM\"].rank(method = \"dense\", ascending = True).astype(int)\n",
    "\n",
    "            if cutoff_point_name == \"SWITCHING TO IS\":\n",
    "\n",
    "                cutoff_point_timestamp = as_orders_rdd_sample_on_cutoff_date[as_orders_rdd_sample_on_cutoff_date[\"STATUS_RANK\"]==1][as_orders_rdd_sample_on_cutoff_date[\"ORDER_ITEM_BACKORDER_FLAG\"] == False].reset_index().iloc[0][\"ORDER_PLACED_DTTM\"]\n",
    "\n",
    "            else:\n",
    "\n",
    "                cutoff_point_timestamp = as_orders_rdd_sample_on_cutoff_date[as_orders_rdd_sample_on_cutoff_date[\"STATUS_RANK\"]==1][as_orders_rdd_sample_on_cutoff_date[\"ORDER_ITEM_BACKORDER_FLAG\"] == True].reset_index().iloc[0][\"ORDER_PLACED_DTTM\"]\n",
    "\n",
    "            # [2] Treatment variable    \n",
    "            \n",
    "            as_orders_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] = np.where(as_orders_rdd_sample_cleaned[\"ORDER_ITEM_BACKORDER_FLAG\"] == False, 1, 0)\n",
    "\n",
    "            # [3] Generate the hour difference between the timestamp and the cutoff point\n",
    "            \n",
    "            # We make the following update: We are going to drop data points that are \"not consistent with RDD design\"\n",
    "           \n",
    "            as_orders_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] = (as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM\"] - cutoff_point_timestamp).dt.total_seconds()/3600\n",
    "\n",
    "            if cutoff_point_name == \"SWITCHING TO IS\":\n",
    "\n",
    "                as_orders_rdd_sample_cleaned[\"DROP_INDICATOR\"] = np.where( ((as_orders_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 1) & (as_orders_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] < 0)) \\\n",
    "                                                                              | ((as_orders_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 0) & (as_orders_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] >= 0)), 1, 0)\n",
    "          \n",
    "                # print(\"BEFORE DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(as_orders_rdd_sample_cleaned)))\n",
    "                \n",
    "                as_orders_rdd_sample_cleaned = as_orders_rdd_sample_cleaned[as_orders_rdd_sample_cleaned[\"DROP_INDICATOR\"]==0]\n",
    "                \n",
    "                # print(\"AFTER DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(as_orders_rdd_sample_cleaned)))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                as_orders_rdd_sample_cleaned[\"DROP_INDICATOR\"] = np.where( ((as_orders_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 1) & (as_orders_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] >= 0)) \\\n",
    "                                                                              | ((as_orders_rdd_sample_cleaned[\"TREATMENT_VARIABLE\"] == 0) & (as_orders_rdd_sample_cleaned[\"RUNNING_VARIABLE\"] < 0)), 1, 0)\n",
    "          \n",
    "                # print(\"BEFORE DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(as_orders_rdd_sample_cleaned)))\n",
    "                \n",
    "                as_orders_rdd_sample_cleaned = as_orders_rdd_sample_cleaned[as_orders_rdd_sample_cleaned[\"DROP_INDICATOR\"]==0]\n",
    "                \n",
    "                # print(\"AFTER DROPPING THE INCONSISTENT DATA POINTS, # OF DATA POINTS IS: \" + str(len(as_orders_rdd_sample_cleaned)))\n",
    "            \n",
    "            # [4] Outcome Variable Construction\n",
    "\n",
    "            customer_id_list_in_as_orders_rdd_sample = as_orders_rdd_sample_cleaned[[\"CUSTOMER_ID\"]].drop_duplicates()[\"CUSTOMER_ID\"].to_list()\n",
    "\n",
    "            # Add filtering variables\n",
    "            as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM_UB\"] = as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM\"] + pd.Timedelta(hours=24)\n",
    "            as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM_END\"] = as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM\"] + pd.Timedelta(hours=24*365)\n",
    "\n",
    "            # Max timestamp possible for CP calculation\n",
    "            max_timestamp = np.max(as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM_END\"])\n",
    "            # Min timestamp possible for CP calculation\n",
    "            min_timestamp = np.min(as_orders_rdd_sample_cleaned[\"ORDER_PLACED_DTTM\"])\n",
    "\n",
    "            # Focus on customers in clickstream data & Focus on time period that is relevant\n",
    "            df_cp_as_orders = df_cp.filter( (F.col(\"CUSTOMER_ID\").isin(customer_id_list_in_as_orders_rdd_sample)) & \\\n",
    "                                            (F.col(\"ORDER_PLACED_DTTM\") >= min_timestamp) & \\\n",
    "                                            (F.col(\"ORDER_PLACED_DTTM\") <= max_timestamp) )\n",
    "            # df_cp_as_orders = df_cp_as_orders.na.fill(value=0,subset=[\"IB_COST\", \"OB_COST\", \"COGS\", \"AVERAGE_PRICE\"])\n",
    "            # df_cp_as_orders = df_cp_as_orders.withColumn(\"NET_MARGIN\", (F.col(\"AVERAGE_PRICE\")-F.col(\"IB_COST\")-F.col(\"OB_COST\")-F.col(\"COGS\"))*F.col(\"ORDER_LINE_QUANTITY\") )\n",
    "\n",
    "            df_cp_as_orders = df_cp_as_orders.withColumn(\"NET_MARGIN_V2\",  F.col(\"AVERAGE_PRICE\")-F.col(\"IB_COST\")-F.col(\"OB_COST\")-F.col(\"COGS\"))\n",
    "\n",
    "            # Transform the small cp data into pandas dataframe\n",
    "            print(\"==========Converting CP data into Pandas data frame==========\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            pd_cp_as_orders = df_cp_as_orders.toPandas()\n",
    "            pd_cp_as_orders[\"ORDER_LINE_QUANTITY\"] = pd_cp_as_orders[\"ORDER_LINE_QUANTITY\"].astype(float)\n",
    "            # CP data construction: keep the relevant columns\n",
    "            cp_relevant_columns = [\"CUSTOMER_ID\", \"PRODUCT_PART_NUMBER_IN_CP\", \"ORDER_PLACED_DTTM_CP\", \"ORDER_LINE_QUANTITY\", \"IS_AUTO_SHIP\", \"NET_MARGIN\", \"NET_MARGIN_V2\"]\n",
    "            pd_cp_as_orders_adj = pd_cp_as_orders[cp_relevant_columns]\n",
    "            \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            # Keep the relevant columns of scheduled as orders data\n",
    "            as_orders_relevant_columns = as_orders_identifier_variables + [\"RUNNING_VARIABLE\", \"TREATMENT_VARIABLE\",\"ORDER_PLACED_DTTM_UB\", \"ORDER_PLACED_DTTM_END\"]\n",
    "            as_orders_rdd_sample_cleaned_adj = as_orders_rdd_sample_cleaned[as_orders_relevant_columns]\n",
    "\n",
    "            start_time_0 = time.time()\n",
    "\n",
    "            # alternative way\n",
    "            for row in range(len(as_orders_rdd_sample_cleaned_adj)):\n",
    "\n",
    "                # print(\"==========NOW GENERATING CP FOR DATA POINT: \" + str(row) + \" ==========\")\n",
    "\n",
    "                # start_time = time.time()\n",
    "\n",
    "                # Loop through each as order data point\n",
    "\n",
    "                as_order_dp = as_orders_rdd_sample_cleaned_adj.iloc[row:(row+1),:].reset_index()\n",
    "\n",
    "                # info needed to be added to CP data\n",
    "                customer_id = as_order_dp[\"CUSTOMER_ID\"][0]\n",
    "                order_id = as_order_dp[\"ORDER_ID\"][0]\n",
    "                order_line_id = as_order_dp[\"ORDER_LINE_ID\"][0]\n",
    "                product_part_number = as_order_dp[\"PRODUCT_PART_NUMBER\"][0]\n",
    "                timestamp_lb = as_order_dp[\"ORDER_PLACED_DTTM\"][0]\n",
    "                timestamp_ub = as_order_dp[\"ORDER_PLACED_DTTM_UB\"][0]\n",
    "                timestamp_end = as_order_dp[\"ORDER_PLACED_DTTM_END\"][0]\n",
    "                order_date = as_order_dp[\"ORDER_PLACED_DATE\"][0]\n",
    "                order_year = as_order_dp[\"ORDER_PLACED_YEAR\"][0]\n",
    "                order_month = as_order_dp[\"ORDER_PLACED_MONTH\"][0]\n",
    "                instock_status = as_order_dp[\"ORDER_ITEM_BACKORDER_FLAG\"][0]\n",
    "                order_status = as_order_dp[\"ORDER_STATUS\"][0]\n",
    "                running_variable = as_order_dp[\"RUNNING_VARIABLE\"][0]\n",
    "                treatment_variable = as_order_dp[\"TREATMENT_VARIABLE\"][0]\n",
    "\n",
    "                # get CP data that corresponds to the customer id of the clickstream dp\n",
    "                pd_cp_as_orders_dp = pd_cp_as_orders_adj[pd_cp_as_orders_adj[\"CUSTOMER_ID\"] == customer_id]\n",
    "\n",
    "                # add the info from the as order dp into CP data\n",
    "                pd_cp_as_orders_dp[\"PRODUCT_PART_NUMBER\"] = product_part_number\n",
    "                pd_cp_as_orders_dp[\"ORDER_ID\"] = order_id\n",
    "                pd_cp_as_orders_dp[\"ORDER_LINE_ID\"] = order_line_id\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_DTTM\"] = timestamp_lb\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_DTTM_UB\"] = timestamp_ub\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_DTTM_END\"] = timestamp_end\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_DATE\"] = order_date\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_YEAR\"] = order_year\n",
    "                pd_cp_as_orders_dp[\"ORDER_PLACED_MONTH\"] = order_month\n",
    "                pd_cp_as_orders_dp[\"ORDER_ITEM_BACKORDER_FLAG\"] = instock_status\n",
    "                pd_cp_as_orders_dp[\"ORDER_STATUS\"] = order_status\n",
    "                pd_cp_as_orders_dp[\"RUNNING_VARIABLE\"] = running_variable\n",
    "                pd_cp_as_orders_dp[\"TREATMENT_VARIABLE\"] = treatment_variable\n",
    "\n",
    "                # if len(pd_cp_as_orders_dp) == 0:\n",
    "                #    print(\"THIS DATA POINT HAS NO CP RECORD\")\n",
    "\n",
    "                # else:\n",
    "                #    print(\"THIS DATA POINT HAS CP RECORD: \" + str(len(pd_cp_as_orders_dp)))\n",
    "\n",
    "                pd_cp_as_orders_dp[\"CP_INCLUDED\"] = np.where(\n",
    "\n",
    "                    ((pd_cp_as_orders_dp['PRODUCT_PART_NUMBER_IN_CP'] != pd_cp_as_orders_dp['PRODUCT_PART_NUMBER']) & (pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_as_orders_dp['ORDER_PLACED_DTTM']) & (pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_as_orders_dp['ORDER_PLACED_DTTM_UB']))\\\n",
    "                    | \\\n",
    "\n",
    "                    ((pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_as_orders_dp['ORDER_PLACED_DTTM_UB']) & (pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_as_orders_dp['ORDER_PLACED_DTTM_END']))\n",
    "\n",
    "                    , 1, 0)                \n",
    "                \n",
    "                pd_cp_as_orders_dp[\"CP_INCLUDED_V2\"] = np.where(\n",
    "\n",
    "                (pd_cp_as_orders_dp['PRODUCT_PART_NUMBER_IN_CP'] != pd_cp_as_orders_dp['PRODUCT_PART_NUMBER']) \\\n",
    "                 & ((pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] >= pd_cp_as_orders_dp['ORDER_PLACED_DTTM']) \\\n",
    "                    & (pd_cp_as_orders_dp['ORDER_PLACED_DTTM_CP'] < pd_cp_as_orders_dp['ORDER_PLACED_DTTM_END']))\n",
    "\n",
    "                , 1, 0)\n",
    "\n",
    "                pd_cp_as_orders_dp[\"CP\"] = pd_cp_as_orders_dp[\"CP_INCLUDED\"] * pd_cp_as_orders_dp[\"NET_MARGIN\"] * pd_cp_as_orders_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                pd_cp_as_orders_dp[\"CP_V2\"] = pd_cp_as_orders_dp[\"CP_INCLUDED\"] * pd_cp_as_orders_dp[\"NET_MARGIN_V2\"] * pd_cp_as_orders_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                pd_cp_as_orders_dp[\"CP_V3\"] = pd_cp_as_orders_dp[\"CP_INCLUDED_V2\"] * pd_cp_as_orders_dp[\"NET_MARGIN\"] * pd_cp_as_orders_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                pd_cp_as_orders_dp[\"CP_V4\"] = pd_cp_as_orders_dp[\"CP_INCLUDED_V2\"] * pd_cp_as_orders_dp[\"NET_MARGIN_V2\"] * pd_cp_as_orders_dp[\"ORDER_LINE_QUANTITY\"]\n",
    "                \n",
    "                groupVars = as_orders_relevant_columns\n",
    "\n",
    "                pd_agg_cp_as_orders_dp = pd_cp_as_orders_dp.groupby(groupVars)[[\"CP\", \"CP_V2\", \"CP_V3\", \"CP_V4\"]].sum().reset_index()\n",
    "\n",
    "                if row == 0:\n",
    "\n",
    "                    pd_as_orders_cp_outcome =  pd_agg_cp_as_orders_dp\n",
    "\n",
    "                else:\n",
    "\n",
    "                    pd_as_orders_cp_outcome = pd_as_orders_cp_outcome.append(pd_agg_cp_as_orders_dp)\n",
    "\n",
    "                # print(\"==========GENERATION IS COMPLETED FOR DATA POINT: \" + str(row) + \" ==========\")\n",
    "                # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            # print(\"--- TOTAL --- %s seconds ---\" % (time.time() - start_time_0))\n",
    "\n",
    "            as_orders_rdd_sample_cleaned_adj[\"CUSTOMER_ID\"] = as_orders_rdd_sample_cleaned_adj[\"CUSTOMER_ID\"].astype(str)\n",
    "            pd_as_orders_cp_outcome[\"CUSTOMER_ID\"] = pd_as_orders_cp_outcome[\"CUSTOMER_ID\"].astype(str)\n",
    "            rdd_sample = pd.merge(as_orders_rdd_sample_cleaned_adj, pd_as_orders_cp_outcome, on = groupVars, how = \"left\")\n",
    "    \n",
    "            second_layer_index = str(cutoff_point) + \"_\" + str(cutoff_point_name) + \"_\" + str(window_length)\n",
    "            print(\"CP NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V2 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V2\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V3 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V3\"].isna())/len(rdd_sample)))\n",
    "            print(\"CP_V4 NULL VALUE COUNTS FOR \" + first_layer_index + \"_\" + second_layer_index + \" IS \" + str(sum(rdd_sample[\"CP_V4\"].isna())/len(rdd_sample)))\n",
    "\n",
    "            rdd_sample = rdd_sample.fillna({\"CP\":0,\n",
    "                                           \"CP_V2\":0,\n",
    "                                            \"CP_V3\":0,\n",
    "                                            \"CP_V4\":0})\n",
    "\n",
    "            as_orders_rdd_sample_collection_temp[second_layer_index] = rdd_sample\n",
    "        \n",
    "        \n",
    "    print(\"--- TOTAL RUNTIME FOR RDD CONSTRUCTION OF\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "        \n",
    "    return as_orders_rdd_sample_collection_temp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6049223a-9b6d-41e5-b653-2f8776e39244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 23. AS_ORDERS RDD\n",
    "def as_orders_rdd_modeling(as_orders_rdd_sample_collection, first_layer_index, cp_variable, K, regression_type, h, a1, a2, bootstrapping_method, boostramp_sample_size):\n",
    "\n",
    "    as_orders_rdd_estimate_collection = {}\n",
    "    \n",
    "    print(\"========== RUNNING AS_ORDERS RDD FOR \" + first_layer_index + \" ==========\")\n",
    "    \n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    if regression_type == \"SEPARATE_UNWEIGHTED\":\n",
    "\n",
    "        for sample_index, pd_rdd_sample in as_orders_rdd_sample_collection.items():\n",
    "            print(\"========== NOW RUNNING SEPARATE_UNWEIGHTED REGRESSION FOR SAMPLE: \" + str(sample_index) + \"==========\")\n",
    "\n",
    "            print(\"DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "            pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "            pd_rdd_sample\n",
    "\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "                \n",
    "                plt.figure(0)\n",
    "\n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "                plt.figure(1)\n",
    "            \n",
    "                # another version of the graph\n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "\n",
    "                \n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()        \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show() \n",
    "\n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "            K = 1\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "\n",
    "                results = sm.OLS(Y, X).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(AS_ORDERS) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "\n",
    "                estimate_list = []\n",
    "                for s in range(boostramp_sample_size):\n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "                    \n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.OLS(Y, X).fit()\n",
    "                    \n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                    \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "            as_orders_rdd_estimate_collection[sample_index] = {}\n",
    "            as_orders_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "            as_orders_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\n",
    "\n",
    "    elif regression_type == \"SEPARATE_WEIGHTED\":\n",
    "\n",
    "        for sample_index, pd_rdd_sample in as_orders_rdd_sample_collection.items():\n",
    "            print(\"========== NOW RUNNING SEPARATE_WEIGHTED REGRESSION FOR SAMPLE: \" + str(sample_index) + \"==========\")\n",
    "\n",
    "            print(\"DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "            pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "            pd_rdd_sample\n",
    "\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "            pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "\n",
    "                \n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()      \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                \n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "                \n",
    "                plt.show() \n",
    "\n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(AS_ORDERS) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                estimate_list = []\n",
    "                \n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "            as_orders_rdd_estimate_collection[sample_index] = {}\n",
    "            as_orders_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "            as_orders_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\t\t\n",
    "    elif regression_type == \"JOINT_WEIGHTED\":\n",
    "\n",
    "        index = 1\n",
    "        \n",
    "        for sample_index_temp, pd_rdd_sample_temp in as_orders_rdd_sample_collection.items():\n",
    "            \n",
    "            print(\"========== NOW RUNNING JOINT_WEIGHTED REGRESSION ==========\")\n",
    "            print(\"FOR SAMPLE \" + sample_index_temp + \", DATA POINTS: \" + str(len(pd_rdd_sample_temp)))\n",
    "\n",
    "            pd_rdd_sample_temp[\"SAMPLE_INDEX\"] = index\n",
    "\n",
    "            if index == 1:\n",
    "\n",
    "                pd_rdd_sample = pd_rdd_sample_temp\n",
    "                sample_index = sample_index_temp\n",
    "\n",
    "            else:\n",
    "\n",
    "                pd_rdd_sample = pd_rdd_sample.append(pd_rdd_sample_temp)\n",
    "                sample_index = sample_index_temp\n",
    "\n",
    "            index = index + 1\n",
    "            \n",
    "            print(\"JOINT SAMPLE HAS DATA POINTS: \" + str(len(pd_rdd_sample)))\n",
    "\n",
    "\n",
    "        print(\"========== STEP 1: FURTHER FEATURE ENGINEERING: REMOVE NULL DATA POINTS ==========\")  \n",
    "        pd_rdd_sample = pd_rdd_sample[~pd_rdd_sample[cp_variable].isna()]\n",
    "        pd_rdd_sample\n",
    "\n",
    "        print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "        print(\"========== STEP 2: FURTHER FEATURE ENGINEERING: ADD NON-LINEARITY==========\")\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] \n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"RUNNING_VARIABLE\"]\n",
    "\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_1\"] = pd_rdd_sample[\"RUNNING_VARIABLE_1\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_2\"] = pd_rdd_sample[\"RUNNING_VARIABLE_2\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_3\"] = pd_rdd_sample[\"RUNNING_VARIABLE_3\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "        pd_rdd_sample[\"RUNNING_x_TREATMENT_VARIABLE_4\"] = pd_rdd_sample[\"RUNNING_VARIABLE_4\"] * pd_rdd_sample[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "\n",
    "\n",
    "        if np.max(pd_rdd_sample[\"SAMPLE_INDEX\"]) == 1:\n",
    "            \n",
    "            print(\"NO NEED TO JOIN AND PROCEED AS SEPARATE REGRESSION\")\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\") \n",
    "            \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: VISUALIZATION==========\")\n",
    "            if sample_index.split(\"_\")[1] == \"SWITCHING TO IS\":\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "                # another version of the graph\n",
    "                plt.figure(1)\n",
    "                \n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")   \n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "\n",
    "                plt.figure(0)\n",
    "                \n",
    "                x1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][\"RUNNING_VARIABLE_1\"]\n",
    "                y1 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]<=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==1)][cp_variable]\n",
    "                ax = sns.regplot(x1, y1,\n",
    "\n",
    "                           scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "                           )\n",
    "\n",
    "                x2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][\"RUNNING_VARIABLE_1\"]\n",
    "                y2 = pd_rdd_sample[(pd_rdd_sample[\"RUNNING_VARIABLE_1\"]>=0) & (pd_rdd_sample[\"TREATMENT_VARIABLE\"]==0)][cp_variable]\n",
    "                ax = sns.regplot(x2, y2,\n",
    "\n",
    "                            scatter_kws = {\"alpha\": 0.33},\n",
    "\n",
    "\n",
    "                           )\n",
    "\n",
    "                plt.axvline(0, min(pd_rdd_sample[cp_variable]), max(pd_rdd_sample[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                ax.set(xlabel = \"X: Running Variable (Hour Relative to Cutoff Point)\", ylabel = \"Y: Contribution Profit ($)\")\n",
    "\n",
    "                ax.set(title = \"RDD Visualization For \" + first_layer_index + \"_\" + sample_index)\n",
    "\n",
    "                plt.show()        \n",
    "                \n",
    "                # another version of the graph\n",
    "                \n",
    "                plt.figure(1)\n",
    "                edges = np.linspace(-h,h,2*h+1)\n",
    "                pd_rdd_sample['Bins'] = pd.cut(pd_rdd_sample['RUNNING_VARIABLE_1'], bins = edges)\n",
    "\n",
    "                # Mean within bins\n",
    "                binned = pd_rdd_sample.groupby(['Bins']).agg('mean')\n",
    "\n",
    "                # And plot\n",
    "                sns.lineplot(x = binned['RUNNING_VARIABLE_1'],\n",
    "                          y = binned[cp_variable], marker = 'o')\n",
    "\n",
    "                # Add vertical line at cutoff\n",
    "                plt.axvline(0, min(binned[cp_variable]), max(binned[cp_variable]), c = \"red\",  linestyle=\"dashed\")\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "            print(\"========== STEP 6: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(AS_ORDERS) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "\n",
    "                estimate_list = []\n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            index_var_name_list = []\n",
    "            \n",
    "            for m in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1):\n",
    "                \n",
    "                index_var_name = \"SAMPLE_INDEX\" + \"_\" + str(m)\n",
    "                pd_rdd_sample[index_var_name] = np.where(pd_rdd_sample[\"SAMPLE_INDEX\"] == m, 1, 0)\n",
    "                index_var_name_list = index_var_name_list + [index_var_name]\n",
    "\n",
    "            vars_for_interaction_with_sample_index = [\"RUNNING_VARIABLE_1\", \"RUNNING_VARIABLE_2\", \"RUNNING_VARIABLE_3\", \"RUNNING_VARIABLE_4\",\n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_1\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_2\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_3\", \n",
    "                                                      \"RUNNING_x_TREATMENT_VARIABLE_4\"]\n",
    "            \n",
    "            interaction_var_name_list = []\n",
    "            \n",
    "            for m1 in index_var_name_list:\n",
    "                for m2 in vars_for_interaction_with_sample_index:\n",
    "                    \n",
    "                    interaction_var_name = m2 + \"_\" + m1\n",
    "                    pd_rdd_sample[interaction_var_name] = pd_rdd_sample[m1] * pd_rdd_sample[m2]\n",
    "                    interaction_var_name_list = interaction_var_name_list + [interaction_var_name]\n",
    "\n",
    "\n",
    "            print(\"========== STEP 3: FURTHER FEATURE ENGINEERING: WINDOW LENGTH ADJUSTMENT==========\")   \n",
    "            rdd_window_lb = min(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "            rdd_window_ub = max(pd_rdd_sample[\"RUNNING_VARIABLE\"])\n",
    "\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[\"RUNNING_VARIABLE\"] >= rdd_window_lb) & (pd_rdd_sample[\"RUNNING_VARIABLE\"] <= rdd_window_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 4: FILTERING OUT EXTREME CP VALUES==========\")   \n",
    "            lb_perc = .025\n",
    "            ub_perc = .975\n",
    "            perc_lb = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[lb_perc]\n",
    "            perc_ub = pd_rdd_sample[cp_variable].quantile([lb_perc, ub_perc])[ub_perc]\n",
    "            pd_rdd_sample = pd_rdd_sample[ (pd_rdd_sample[cp_variable] >= perc_lb) & (pd_rdd_sample[cp_variable] <= perc_ub) ]\n",
    "            print(\"RESULTING DATA POINTS: \" +str(len(pd_rdd_sample)))\n",
    "\n",
    "            print(\"========== STEP 5: REGRESSION==========\")\n",
    "\n",
    "            # ORDER OF POLYNOMIAL\n",
    "\n",
    "            var_list = [\"TREATMENT_VARIABLE\"] + [\"RUNNING_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] \\\n",
    "            + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n) for n in np.arange(1,K+1)] \\\n",
    "            + [\"RUNNING_VARIABLE_\" + str(n1)  + \"_\" + \"SAMPLE_INDEX_\" + str(n2) for n1 in np.arange(1,K+1) for n2 in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1)] \\\n",
    "            + [\"RUNNING_x_TREATMENT_VARIABLE_\" + str(n1)  + \"_\" + \"SAMPLE_INDEX_\" + str(n2) for n1 in np.arange(1,K+1) for n2 in np.arange(2, np.max(pd_rdd_sample[\"SAMPLE_INDEX\"])+1)]\n",
    "\n",
    "            if bootstrapping_method == False:\n",
    "\n",
    "                X = pd_rdd_sample[var_list]\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                Y = pd_rdd_sample[[cp_variable]]\n",
    "\n",
    "                results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "\n",
    "                print(results.summary())\n",
    "\n",
    "                rdd_estimate_value = results.params[\"TREATMENT_VARIABLE\"]\n",
    "                rdd_estimate_std_err = results.bse[\"TREATMENT_VARIABLE\"]\n",
    "\n",
    "                print(\"(AS_ORDERS) THE ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                estimate_list = []\n",
    "                \n",
    "                for s in range(boostramp_sample_size):\n",
    "                    \n",
    "                    pd_rdd_sample_resampled = pd_rdd_sample.sample(n = len(pd_rdd_sample), replace = True)\n",
    "                    \n",
    "                    X = pd_rdd_sample_resampled[var_list]\n",
    "                    X = sm.add_constant(X)\n",
    "\n",
    "                    Y = pd_rdd_sample_resampled[[cp_variable]]\n",
    "                    \n",
    "                    results = sm.WLS(Y,X, weights= kernel(X[\"RUNNING_VARIABLE_1\"], h, a1, a2)).fit()\n",
    "                    estimate_list = estimate_list + [results.params[\"TREATMENT_VARIABLE\"]]\n",
    "                    \n",
    "                plt.hist(np.array(estimate_list))\n",
    "                plt.show()\n",
    "                \n",
    "                rdd_estimate_value = np.mean(np.array(estimate_list))\n",
    "                rdd_estimate_std_err = np.std(np.array(estimate_list))\n",
    "                \n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED ESTIMATED TREATMENT EFFECT FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_value))\n",
    "                print(\"(AS_ORDERS) THE BOOSTRAPPED STANDARD ERROR OF THE ESTIMATE FOR \" + first_layer_index + \"_\" + str(sample_index) + \" IS: \" + str(rdd_estimate_std_err))\n",
    "                \n",
    "        as_orders_rdd_estimate_collection[sample_index] = {}\n",
    "        as_orders_rdd_estimate_collection[sample_index][\"Estimate\"] = rdd_estimate_value\n",
    "        as_orders_rdd_estimate_collection[sample_index][\"StdError\"] = rdd_estimate_std_err\n",
    "\n",
    "    print(\"--- TOTAL RUNTIME OF AS_ORDERS RDD FOR\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "        \n",
    "    return as_orders_rdd_estimate_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2164f483-3e8a-42b9-945c-42455d9f2107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 24. AS orders RDD result summaries\n",
    "def as_orders_rdd_result_summary(as_orders_rdd_estimate_collection, first_layer_index):\n",
    "    \n",
    "    print(\"========== SUMMARIZING AS_ORDERS RDD RESULTS FOR \" + first_layer_index + \" ==========\")\n",
    "    start_time_00 = time.time()\n",
    "    \n",
    "    as_orders_rdd_estimate_value_list = []\n",
    "    as_orders_rdd_estimate_std_error_list = []\n",
    "\n",
    "    for result_index, result_sample in as_orders_rdd_estimate_collection.items():\n",
    "\n",
    "        as_orders_rdd_estimate_value_list += [result_sample[\"Estimate\"]]\n",
    "        as_orders_rdd_estimate_std_error_list += [result_sample[\"StdError\"]]\n",
    "\n",
    "    if len(as_orders_rdd_estimate_value_list) == 1:\n",
    "        final_as_orders_estimate_value = as_orders_rdd_estimate_value_list[0]\n",
    "        final_as_orders_estimate_std_error = as_orders_rdd_estimate_std_error_list[0]\n",
    "        final_as_orders_z_score = final_as_orders_estimate_value/final_as_orders_estimate_std_error\n",
    "        final_as_orders_statistical_significance_p05 = np.abs(final_as_orders_z_score) >= 1.96\n",
    "        final_as_orders_statistical_significance_p10 = np.abs(final_as_orders_z_score) >= 1.645\n",
    "        print(\"(AS_ORDERS) FINAL RDD ESTIMATE FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_estimate_value))\n",
    "        print(\"(AS_ORDERS) FINAL RDD ESTIMATE STD ERROR FOR \" + first_layer_index + \" IS: \"+ str(final_as_orders_estimate_std_error))\n",
    "        print(\"(AS_ORDERS) FINAL Z SCORE FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_z_score))\n",
    "        print(\"(AS_ORDERS) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 5% FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_statistical_significance_p05))\n",
    "        print(\"(AS_ORDERS) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 10% FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_statistical_significance_p10))\n",
    "\n",
    "    else:\n",
    "\n",
    "        np_as_orders_rdd_estimate_values = np.array(as_orders_rdd_estimate_value_list)\n",
    "        np_as_orders_rdd_estimate_values_2 = np_as_orders_rdd_estimate_values**2\n",
    "        np_as_orders_rdd_estimate_std_errors = np.array(as_orders_rdd_estimate_std_error_list)\n",
    "        np_as_orders_rdd_estimate_variance = np_as_orders_rdd_estimate_std_errors**2\n",
    "\n",
    "        # assign equal weights to each estimate\n",
    "        as_orders_np_weights = np.array([1/len(as_orders_rdd_estimate_value_list)] * len(as_orders_rdd_estimate_value_list))\n",
    "\n",
    "        # final_estimate_value = np.sum(np.array(rdd_estimate_value_list)*np_optimal_weights)\n",
    "        final_as_orders_estimate_value = np.sum(np_as_orders_rdd_estimate_values * as_orders_np_weights) \n",
    "        final_as_orders_estimate_variance =  np.sum(np_as_orders_rdd_estimate_variance * as_orders_np_weights)\\\n",
    "        + np.sum(np_as_orders_rdd_estimate_values_2 * as_orders_np_weights)\\\n",
    "        - np.square(final_as_orders_estimate_value)\n",
    "        final_as_orders_estimate_std_error = np.sqrt(final_as_orders_estimate_variance)\n",
    "\n",
    "        final_as_orders_z_score = final_as_orders_estimate_value / final_as_orders_estimate_std_error\n",
    "        final_as_orders_statistical_significance_p05 = np.abs(final_as_orders_z_score) >= 1.96\n",
    "        final_as_orders_statistical_significance_p10 = np.abs(final_as_orders_z_score) >= 1.645\n",
    "        print(\"(AS_ORDERS) FINAL RDD ESTIMATE FOR: \" + str(final_as_orders_estimate_value))\n",
    "        print(\"(AS_ORDERS) FINAL RDD ESTIMATE STD ERROR FOR \" + str(final_as_orders_estimate_std_error))\n",
    "        print(\"(AS_ORDERS) FINAL Z SCORE FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_z_score))\n",
    "        print(\"(AS_ORDERS) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 5% FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_statistical_significance_p05))\n",
    "        print(\"(AS_ORDERS) FINAL STATISTICAL SIGNIFICANCE CONFIDENCE LEVEL 10% FOR \" + first_layer_index + \" IS: \" + str(final_as_orders_statistical_significance_p10))\n",
    "        \n",
    "    finalized_as_orders_rdd_estimates = {}\n",
    "    finalized_as_orders_rdd_estimates[\"Estimate\"] = final_as_orders_estimate_value\n",
    "    finalized_as_orders_rdd_estimates[\"StdError\"] = final_as_orders_estimate_std_error\n",
    "    finalized_as_orders_rdd_estimates[\"ZScore\"] = final_as_orders_z_score\n",
    "    finalized_as_orders_rdd_estimates[\"Significance_5_perc\"] = final_as_orders_statistical_significance_p05\n",
    "    finalized_as_orders_rdd_estimates[\"Significance_10_perc\"] = final_as_orders_statistical_significance_p10 \n",
    "   \n",
    "    print(\"--- TOTAL RUNTIME OF AS_ORDERS RDD RESULT SUMMARY FOR\" + first_layer_index + \"IS \" +  str(time.time() - start_time_00))\n",
    "    \n",
    "    return finalized_as_orders_rdd_estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deaa5bf9-e8ff-40eb-88c0-ffc2725531f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Generate pd_valid_as_orders_rdd_estimates\n",
    "def generate_pd_valid_as_orders_rdd_estimates(finalized_as_orders_rdd_estimates):\n",
    "    \n",
    "    k = 0\n",
    "\n",
    "    for key, value in finalized_as_orders_rdd_estimates.items():\n",
    "\n",
    "        key_list = key.split(\"_\")\n",
    "        Product = key_list[0]\n",
    "        Year = key_list[1]\n",
    "        Month = key_list[2]\n",
    "\n",
    "        Estimate = value[\"Estimate\"]\n",
    "        StdError = value[\"StdError\"]\n",
    "        ZScore = value[\"ZScore\"]\n",
    "        Significance_5_perc = value[\"Significance_5_perc\"]\n",
    "        Significance_10_perc = value[\"Significance_10_perc\"]\n",
    "\n",
    "        if Significance_10_perc == True:\n",
    "            if k == 0:\n",
    "                valid_rdd_estimates_data = [[Product, Year, Month, Estimate, StdError]]\n",
    "            else:\n",
    "                valid_rdd_estimates_data = valid_rdd_estimates_data +  [[Product, Year, Month, Estimate, StdError]]\n",
    "\n",
    "            k = k + 1\n",
    "\n",
    "    pd_valid_rdd_estimates = pd.DataFrame(valid_rdd_estimates_data, columns = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\", \"ESTIMATE\", \"STD_ERROR\"])\n",
    "    pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"] = pd_valid_rdd_estimates[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "    return pd_valid_rdd_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10aad14b-cc21-4eee-879a-78e9e78f7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Generate time values & product x time values\n",
    "def generate_as_orders_productxtime_sets(year_list, month_list, product_list, pd_sku_features):\n",
    "    \n",
    "    # 3.2.1. time values \n",
    "    time_values = [[m1, m2] for m1 in year_list for m2 in month_list]\n",
    "    pd_time_values = pd.DataFrame(time_values, columns = [\"YEAR\", \"MONTH\"])\n",
    "    pd_time_values[\"YEAR\"] = pd_time_values[\"YEAR\"].astype(str)\n",
    "    pd_time_values[\"MONTH\"] = pd_time_values[\"MONTH\"].astype(str)\n",
    "\n",
    "    # 3.2.2. product-time values \n",
    "    product_x_time_values = [[m1,m2,m3] for m1 in product_list for m2 in year_list for m3 in month_list]\n",
    "    pd_product_x_time_values = pd.DataFrame(product_x_time_values, columns = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\"])\n",
    "    pd_product_x_time_values[\"PRODUCT_PART_NUMBER\"] = pd_product_x_time_values[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "    pd_product_x_time_values[\"YEAR\"] = pd_product_x_time_values[\"YEAR\"].astype(str)\n",
    "    pd_product_x_time_values[\"MONTH\"] = pd_product_x_time_values[\"MONTH\"].astype(str)\n",
    "    \n",
    "    # 3.2.3. Add features\n",
    "    pd_product_x_time_with_sku_features = pd_product_x_time_values.merge(pd_sku_features, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\n",
    "    \n",
    "    return pd_time_values, pd_product_x_time_with_sku_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95e1fb1d-158c-4a9a-83aa-cad6d1094ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Remediation: Generate final remediated estimates\n",
    "\n",
    "def generate_as_orders_remediated_estimates(year_list, month_list, pd_time_values, pd_valid_rdd_estimates_with_sku_features, pd_product_x_time_with_sku_features):\n",
    "\n",
    "    k=0\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "\n",
    "            # Get the slice that spans month 0 to the current month \n",
    "            pd_time_values_month0_to_current_month = pd_time_values.iloc[0:(k+1),:]\n",
    "            pd_valid_rdd_estimates_with_sku_features_seed = pd_valid_rdd_estimates_with_sku_features.merge(pd_time_values_month0_to_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Get the slice that covers the current month (for constructing the complete results for the current month)\n",
    "            pd_time_values_current_month = pd_time_values.iloc[k:(k+1),:]\n",
    "            pd_valid_rdd_estimates_with_sku_features_current_month = pd_valid_rdd_estimates_with_sku_features.merge(pd_time_values_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Get the slice that covers the current month (for constructing the complete results for the current month  serving as left side)\n",
    "            pd_product_x_time_with_sku_features_current_month =  pd_product_x_time_with_sku_features.merge(pd_time_values_current_month, on = [\"YEAR\", \"MONTH\"])\n",
    "\n",
    "            # Summarize by SKU\n",
    "            avg_estimate_by_SKU = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"PRODUCT_PART_NUMBER\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_SKU.columns = [\"PRODUCT_PART_NUMBER\", \"SKU_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1-MC2-MC3 combination\n",
    "            avg_estimate_by_MC1_MC2_MC3 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\",\"MC2\",\"MC3\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1_MC2_MC3.columns = [\"MC1\",\"MC2\",\"MC3\", \"MC1_MC2_MC3_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1-MC2 combination\n",
    "            avg_estimate_by_MC1_MC2 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\", \"MC2\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1_MC2.columns = [\"MC1\", \"MC2\", \"MC1_MC2_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Summarize by MC1\n",
    "            avg_estimate_by_MC1 = pd_valid_rdd_estimates_with_sku_features_seed.groupby([\"MC1\"])[\"ESTIMATE\"].mean().reset_index()\n",
    "            avg_estimate_by_MC1.columns = [\"MC1\", \"MC1_AVG_ESTIMATE\"]\n",
    "\n",
    "            # Overall estimate\n",
    "            avg_estimate_overall = np.mean(pd_valid_rdd_estimates_with_sku_features_seed[\"ESTIMATE\"])\n",
    "\n",
    "            # Now we merge the average results with current month results to complete remediation \n",
    "            matching_vars = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\", \"MC1\", \"MC2\", \"MC3\"]\n",
    "            pd_product_x_time_with_sku_features_current_month_remediated = pd_product_x_time_with_sku_features_current_month.merge(pd_valid_rdd_estimates_with_sku_features_current_month, on = matching_vars, how = \"left\")\\\n",
    "            .merge(avg_estimate_by_SKU, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1_MC2_MC3, on = [\"MC1\", \"MC2\", \"MC3\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1_MC2, on = [\"MC1\", \"MC2\"], how = \"left\")\\\n",
    "            .merge(avg_estimate_by_MC1, on = [\"MC1\"], how = \"left\")\n",
    "            pd_product_x_time_with_sku_features_current_month_remediated[\"AVERAGE_ESTIMATE_OVERALL\"] = avg_estimate_overall\n",
    "\n",
    "            # Fill in missing values in the order of organic estimate, SKU average, MC1-MC2-MC3 average, MC1-MC2 average, M1 average, overall average \n",
    "            pd_product_x_time_with_sku_features_current_month_remediated[\"REMEDIATED_ESTIMATE\"] = np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"ESTIMATE\"],\n",
    "                                                                                                          np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"SKU_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"SKU_AVG_ESTIMATE\"],\n",
    "                                                                                                                  np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_MC3_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_MC3_AVG_ESTIMATE\"],\n",
    "                                                                                                                          np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_MC2_AVG_ESTIMATE\"], \n",
    "                                                                                                                                  np.where(~pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_AVG_ESTIMATE\"].isnull(), pd_product_x_time_with_sku_features_current_month_remediated[\"MC1_AVG_ESTIMATE\"], pd_product_x_time_with_sku_features_current_month_remediated[\"AVERAGE_ESTIMATE_OVERALL\"])))))\n",
    "\n",
    "\n",
    "            # Construct the complete estimates from month 0 to current month\n",
    "            if k == 0:\n",
    "                pd_remediated_estimates = pd_product_x_time_with_sku_features_current_month_remediated\t\t\n",
    "            else:\n",
    "                pd_remediated_estimates = pd_remediated_estimates.append(pd_product_x_time_with_sku_features_current_month_remediated)\n",
    "\n",
    "    \n",
    "            k=k+1\n",
    "        \n",
    "    pd_remediated_estimates = pd_remediated_estimates.reset_index()    \n",
    "            \n",
    "    return pd_remediated_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3c8da-796a-4454-a6a0-06aac6005546",
   "metadata": {},
   "source": [
    "# 2. Algorithm Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e88e1-3777-48b3-bd36-d56f783d7864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. CP data\n",
    "cp_data_path = \"s3a://prd-use1-datascientists-sc-fp-data/prd/aero/AERO_ORDER_PLACED_WITH_CP_DATA/\"\n",
    "df_cp = read_cp_data(cp_data_path)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- Complete CP data points: \" + str(df_cp.count()) + \"---\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Part 1. Clickstream RDD\n",
    "# 1. Complete clickstream data\n",
    "clickstream_data_path = \"s3a://prd-use1-datascientists-sc-fp-data/prd/aero/AERO_CLICK_STREAM_SUBSAMPLE/\"\n",
    "df_clickstream_data = read_clickstream_data(clickstream_data_path)\n",
    "start_time = time.time()\n",
    "print(\"--- Complete clickstream data points: \" + str(df_clickstream_data.count()) + \"---\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 2. Complete rdd eligible sku-year-month combinations \n",
    "pd_clickstream_sku_year_month_eligible_complete = pd.read_csv(\"instock_value_clickstream_candidate_sku_months.csv\")\\\n",
    "                                                    .sort_values([\"YEAR\",\"MONTH\",\"PRODUCT_PART_NUMBER\"])\n",
    "pd_clickstream_sku_year_month_eligible_complete[\"YEAR\"] = pd_clickstream_sku_year_month_eligible_complete[\"YEAR\"].astype(str)\n",
    "pd_clickstream_sku_year_month_eligible_complete[\"MONTH\"] = pd_clickstream_sku_year_month_eligible_complete[\"MONTH\"].astype(str)\n",
    "pd_clickstream_sku_year_month_eligible_complete[\"PRODUCT_PART_NUMBER\"] = pd_clickstream_sku_year_month_eligible_complete[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "\n",
    "# \n",
    "pd_sku_features = pd.read_csv(\"sku_features.csv\")\n",
    "\n",
    "# 3. The combinations requested\n",
    "# 3.1. product, year, months requested\n",
    "pd_sku_needed = pd.read_csv(\"Aristotle_AB_Experiment_Item_Selection.csv\")\n",
    "pd_sku_needed.columns = [x.upper() for x in pd_sku_needed.columns.to_list()]\n",
    "clickstream_year_list = [str(x) for x in [2020, 2021, 2022]]\n",
    "clickstream_month_list = [str(x) for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n",
    "clickstream_product_list = pd_sku_needed[\"PRODUCT_PART_NUMBER\"].to_list()\n",
    "# 3.2. product x year x month complete set requested\n",
    "pd_clickstream_time_values, pd_clickstream_product_x_time_with_sku_features = generate_clickstream_productxtime_sets(clickstream_year_list, clickstream_month_list, clickstream_product_list, pd_sku_features)\n",
    "# 3.3. product x year x month requested that is possibly rdd eligible \n",
    "pd_clickstream_sku_year_month_eligible_requested = pd_clickstream_sku_year_month_eligible_complete.merge(pd_clickstream_product_x_time_with_sku_features, on = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\"], how = \"inner\")\n",
    "\n",
    "# 4. \n",
    "#\n",
    "s = 1\n",
    "stopper = 27\n",
    "clickstream_rdd_customers = {}\n",
    "clickstream_rdd_sample_collection = {}\n",
    "for year in clickstream_year_list:\n",
    "    for month in clickstream_month_list:\n",
    "        \n",
    "        # latest result: Feb, 22\n",
    "        if s > stopper:\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Get the subset of the clickstream data\n",
    "            zero_layer_index = str(year) + \"_\" + str(month)\n",
    "\n",
    "\n",
    "            # Filter out the month's data\n",
    "            df_clickstream_data_slice = df_clickstream_data.where((F.col(\"SESSION_YEAR\") == year) \\\n",
    "                                                    & (F.col(\"SESSION_MONTH\") == month))\n",
    "\n",
    "            pd_clickstream_sku_year_month_eligible_requested_monthly_slice = pd_clickstream_sku_year_month_eligible_requested[ (pd_clickstream_sku_year_month_eligible_requested[\"YEAR\"] == year) \\\n",
    "                                                                        & (pd_clickstream_sku_year_month_eligible_requested[\"MONTH\"] == month) ].reset_index()\n",
    "\n",
    "            nNum = len(pd_clickstream_sku_year_month_eligible_requested_monthly_slice)\n",
    "            print(\"(CLICKSTREAM) # OF REQUESTED RDD ELIGIBLE EXAMPLES FOR YEAR \" + str(year) + \" AND MONTH \" + str(month) + \" IS \" + str(nNum))\n",
    "\n",
    "            #\n",
    "            blockSize = 25\n",
    "            m = 0 \n",
    "            block_index = 0\n",
    "            clickstream_rdd_customers[zero_layer_index] = {}\n",
    "\n",
    "            while m <= nNum:\n",
    "\n",
    "                clickstream_rdd_customers_list = []\n",
    "\n",
    "                pd_clickstream_sku_year_month_eligible_requested_monthly_slice_block = pd_clickstream_sku_year_month_eligible_requested_monthly_slice.iloc[m:min(nNum,m+blockSize),:]\n",
    "                product_list = pd_clickstream_sku_year_month_eligible_requested_monthly_slice_block[\"PRODUCT_PART_NUMBER\"].to_list()\n",
    "                df_clickstream_data_slice_block = df_clickstream_data_slice.filter(F.col(\"PRODUCT_PART_NUMBER\").isin(product_list))\n",
    "\n",
    "                start_time = time.time()\n",
    "                print(\"CLICKSTREAM OF \" + zero_layer_index + \" BLOCK HAS \" + str(df_clickstream_data_slice_block.count()))\n",
    "                print(\"--- %s seconds --- \" + str(time.time() - start_time))\n",
    "\n",
    "                # Cache the data for faster computation\n",
    "                df_clickstream_data_slice_block.persist()\n",
    "              \n",
    "                # Get RDD relevant customers for this block of requested examples \n",
    "                start_time_0 = time.time()\n",
    "                start_time_00 = time.time()\n",
    "\n",
    "                pd_clickstream_data_sku_year_month_dict = {}\n",
    "                pd_clickstream_cutoff_point_detection_dict = {}\n",
    "                pd_clickstream_cutoff_points_dict = {}\n",
    "\n",
    "                for ix0, row0 in pd_clickstream_sku_year_month_eligible_requested_monthly_slice.iterrows():\n",
    "\n",
    "                    sku = row0[\"PRODUCT_PART_NUMBER\"]\n",
    "                    first_layer_index = str(sku) + \"_\" + str(year) + \"_\" + str(month)\n",
    "                    df_clickstream_data_sku_year_month, pd_clickstream_data_sku_year_month = clickstream_slice_pandas(df_clickstream_data_slice_block, sku, year, month)\n",
    "                    pd_clickstream_data_sku_year_month_dict[first_layer_index] = pd_clickstream_data_sku_year_month\n",
    "\n",
    "                    pd_clickstream_cutoff_point_detection, pd_clickstream_cutoff_points = find_clickstream_cutoff_dates(pd_clickstream_data_sku_year_month)\n",
    "                    pd_clickstream_cutoff_point_detection_dict[first_layer_index] = pd_clickstream_cutoff_point_detection\n",
    "                    pd_clickstream_cutoff_points_dict[first_layer_index] = pd_clickstream_cutoff_points                \n",
    "\n",
    "                    sufficiency_ratio = 0.5\n",
    "                    initial_window_length = 7\n",
    "                    clickstream_rdd_customers_temp = find_clickstream_rdd_relevant_customers(pd_clickstream_cutoff_points,\n",
    "                                                                                                  pd_clickstream_cutoff_point_detection,\n",
    "                                                                                                  pd_clickstream_data_sku_year_month,\n",
    "                                                                                                  sufficiency_ratio, initial_window_length,\n",
    "                                                                                                  first_layer_index)\n",
    "                    clickstream_rdd_customers_list = [*set(clickstream_rdd_customers_list+clickstream_rdd_customers_temp)]\n",
    "\n",
    "\n",
    "                    if (ix0 > 0) & (ix0 % 10 == 0):\n",
    "                        print(\"(CLICKSTREAM) \" + str(ix0/nNum*100) + \"%\" + \" COMPLETED FOR \" + zero_layer_index)\n",
    "                        print(\"--- %s seconds --- \" + str(time.time() - start_time_00))\n",
    "                        start_time_00 = time.time()\n",
    "\n",
    "\n",
    "                print(\"(CLICKSTREAM) THE NUM. OF RDD RELEVANT CUSTOMERS IN \" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS \" + str(len(clickstream_rdd_customers_list)))\n",
    "                print(\"--- %s seconds --- \" + str(time.time() - start_time_0))\n",
    "                clickstream_rdd_customers[zero_layer_index][\"BLOCK\" + \"_\" + str(block_index)] = clickstream_rdd_customers_list\n",
    "\n",
    "                # save the results (of the month)\n",
    "                # save_file_name = \"clickstream_rdd_customers_\" + zero_layer_index + \".pkl\"\n",
    "\n",
    "                # with open(save_file_name, \"wb\") as fp:\n",
    "                #    pickle.dump(clickstream_rdd_customers, fp)\n",
    "                #    print(\"RDD CUSTOMERS OF\" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS SAVED.\")\n",
    "\n",
    "\n",
    "\n",
    "                # Get the corresponding relevant portion of CP data\n",
    "                df_cp_block = df_cp.filter(F.col(\"CUSTOMER_ID\").isin(clickstream_rdd_customers_list))\n",
    "                \n",
    "                # Cache the data for faster computation\n",
    "                df_cp_block.persist()\n",
    "\n",
    "                start_time = time.time()\n",
    "                print(\"(CLICKSTREAM) THE NUMBER OF RELEVANT CP DATA POINTS OF\" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS \" + str(df_cp_block.count()))\n",
    "                print(\"--- %s seconds ---\" + str(time.time() - start_time))\n",
    "                \n",
    "                i = 0\n",
    "                start_time = time.time()\n",
    "                start_time_00 = time.time()\n",
    "                for ix0, row0 in pd_clickstream_sku_year_month_eligible_requested_monthly_slice.iterrows():\n",
    "\n",
    "                    # timer\n",
    "                    start_time_initial = time.time()\n",
    "\n",
    "                    # SKU, Year, Month\n",
    "                    sku = row0[\"PRODUCT_PART_NUMBER\"]\n",
    "                    first_layer_index = str(sku) + \"_\" + str(year) + \"_\" + str(month)\n",
    "\n",
    "                    print(\"--- WORKING ON INDEX: \" + str(i) + \" EXAMPLE NO. \" + first_layer_index + \" ---\")\n",
    "\n",
    "                    # Part 1. Clickstream Portion of Instock Value  \n",
    "\n",
    "                    # 1. Clickstream slice\n",
    "                    pd_clickstream_data_sku_year_month = pd_clickstream_data_sku_year_month_dict[first_layer_index]\n",
    "                    print(\"CLICKSTREAM COUNTS FOR \" + first_layer_index + \" IS \" + str(len(pd_clickstream_data_sku_year_month)))\n",
    "\n",
    "                    # 3. Clickstream cutoff Date Data\n",
    "                    pd_clickstream_cutoff_point_detection = pd_clickstream_cutoff_point_detection_dict[first_layer_index]\n",
    "                    pd_clickstream_cutoff_points = pd_clickstream_cutoff_points_dict[first_layer_index]     \n",
    "\n",
    "                    print(\"(CLICKSTREAM) CUTOFF POINTS OF \" + first_layer_index + \" IS \")\n",
    "                    print(pd_clickstream_cutoff_points)\n",
    "\n",
    "                    # 4. RDD regression sample construction\n",
    "                    sufficiency_ratio = 0.5 \n",
    "                    initial_window_length = 7\n",
    "                    clickstream_rdd_sample_collection[first_layer_index] = construct_clickstream_rdd_samples(pd_clickstream_cutoff_points, pd_clickstream_cutoff_point_detection, pd_clickstream_data_sku_year_month, sufficiency_ratio, initial_window_length, first_layer_index, df_cp_block)\n",
    "\n",
    "                    # with open('clickstream_rdd_sample_collection_part2.pkl', 'wb') as fp:\n",
    "                    #    pickle.dump(clickstream_rdd_sample_collection, fp)\n",
    "                    #    print('dictionary saved successfully to file')\n",
    "\n",
    "                    # with open('clickstream_rdd_sample_collection_part2.pkl', 'rb') as fp:\n",
    "                    #    clickstream_rdd_sample_collection = pickle.load(fp)\n",
    "                    #    print('Reading the RDD samples')\n",
    "\n",
    "                    # 5. Regression and Result Summaries\n",
    "                    # CP_V3: CP of ALL products OTHER THAN the one of interest\n",
    "                    cp_variable = \"CP_V3\"\n",
    "\n",
    "                    # Polynomial order of running variable\n",
    "                    K = 1\n",
    "                    # Three types: SEPARATE_UNWEIGHTED (Run RDD for each sample with OLS), SEPARATE_WEIGHTED (Run RDD for each sample with WLS), JOINT_WEIGHTED (Run RDD on a combined sample with WLS)\n",
    "                    regression_type = \"JOINT_WEIGHTED\"\n",
    "                    # assign higher weights to data points within 24 hours of the cutoff point\n",
    "                    h = 24\n",
    "                    a1 = 1\n",
    "                    a2 = 2\n",
    "                    bootstrapping_method = True\n",
    "                    boostramp_sample_size = 2500\n",
    "                    if len(clickstream_rdd_sample_collection[first_layer_index]) == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        clickstream_rdd_estimate_collection[first_layer_index] = clickstream_rdd_modeling(clickstream_rdd_sample_collection[first_layer_index], first_layer_index, cp_variable, K, regression_type, h, a1, a2, bootstrapping_method, boostramp_sample_size)\n",
    "                        finalized_clickstream_rdd_estimates[first_layer_index] = clickstream_rdd_result_summary(clickstream_rdd_estimate_collection[first_layer_index], first_layer_index)\n",
    "\n",
    "                    with open(\"finalized_clickstream_rdd_estimates.pkl\", \"wb\") as fp:\n",
    "                        pickle.dump(finalized_clickstream_rdd_estimates, fp)\n",
    "                        print(\"(CLICKSTREAM) RDD ESTIMATES dictionary saved successfully to file\")    \n",
    "                    \n",
    "                    print(\"--- %s seconds ---\" + str(time.time() - start_time_00))\n",
    "                    start_time_00 = time.time()\n",
    "                    i = i + 1\n",
    "                \n",
    "                print(\"--- %s seconds ---\" + str(time.time() - start_time))\n",
    "                m = m + blockSize\n",
    "                block_index = block_index + 1\n",
    "                df_cp_block.unpersist()\n",
    "\n",
    "# Part 2. As_orders RDD\n",
    "# 1. Complete as_orders data\n",
    "as_orders_data_path = \"s3a://prd-use1-datascientists-sc-fp-data/prd/aero/AERO_AS_ORDERS_SUBSAMPLE/\"\n",
    "df_as_orders_data = read_as_orders_data(as_orders_data_path)\n",
    "start_time = time.time()\n",
    "print(\"--- Complete as_orders data points: \" + str(df_as_orders_data.count()) + \"---\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 2. Complete rdd eligible sku-year-month combinations \n",
    "pd_as_orders_sku_year_month_eligible_complete = pd.read_csv(\"instock_value_as_orders_candidate_sku_months.csv\")\\\n",
    "                                                    .sort_values([\"YEAR\",\"MONTH\",\"PRODUCT_PART_NUMBER\"])\n",
    "pd_as_orders_sku_year_month_eligible_complete[\"YEAR\"] = pd_as_orders_sku_year_month_eligible_complete[\"YEAR\"].astype(str)\n",
    "pd_as_orders_sku_year_month_eligible_complete[\"MONTH\"] = pd_as_orders_sku_year_month_eligible_complete[\"MONTH\"].astype(str)\n",
    "pd_as_orders_sku_year_month_eligible_complete[\"PRODUCT_PART_NUMBER\"] = pd_as_orders_sku_year_month_eligible_complete[\"PRODUCT_PART_NUMBER\"].astype(str)\n",
    "\n",
    "# \n",
    "pd_sku_features = pd.read_csv(\"sku_features.csv\")\n",
    "\n",
    "# 3. The combinations requested\n",
    "# 3.1. product, year, months requested\n",
    "pd_sku_needed = pd.read_csv(\"Aristotle_AB_Experiment_Item_Selection.csv\")\n",
    "pd_sku_needed.columns = [x.upper() for x in pd_sku_needed.columns.to_list()]\n",
    "as_orders_year_list = [str(x) for x in [2020, 2021, 2022]]\n",
    "as_orders_month_list = [str(x) for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n",
    "as_orders_product_list = pd_sku_needed[\"PRODUCT_PART_NUMBER\"].to_list()\n",
    "# 3.2. product x year x month complete set requested\n",
    "pd_as_orders_time_values, pd_as_orders_product_x_time_with_sku_features = generate_as_orders_productxtime_sets(as_orders_year_list, as_orders_month_list, as_orders_product_list, pd_sku_features)\n",
    "# 3.3. product x year x month requested that is possibly rdd eligible \n",
    "pd_as_orders_sku_year_month_eligible_requested = pd_as_orders_sku_year_month_eligible_complete.merge(pd_as_orders_product_x_time_with_sku_features, on = [\"PRODUCT_PART_NUMBER\", \"YEAR\", \"MONTH\"], how = \"inner\")\n",
    "\n",
    "# 4. \n",
    "#\n",
    "s = 1\n",
    "stopper = 27\n",
    "as_orders_rdd_customers = {}\n",
    "as_orders_rdd_sample_collection = {}\n",
    "for year in as_orders_year_list:\n",
    "    for month in as_orders_month_list:\n",
    "        \n",
    "        # latest result: Feb, 22\n",
    "        if s > stopper:\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Get the subset of the as_orders data\n",
    "            zero_layer_index = str(year) + \"_\" + str(month)\n",
    "\n",
    "            # Filter out the month's data\n",
    "            df_as_orders_data_slice = df_as_orders_data.where((F.col(\"ORDER_PLACED_YEAR\") == year) \\\n",
    "                                                    & (F.col(\"ORDER_PLACED_MONTH\") == month))\n",
    "\n",
    "            pd_as_orders_sku_year_month_eligible_requested_monthly_slice = pd_as_orders_sku_year_month_eligible_requested[ (pd_as_orders_sku_year_month_eligible_requested[\"YEAR\"] == year) \\\n",
    "                                                                        & (pd_as_orders_sku_year_month_eligible_requested[\"MONTH\"] == month) ].reset_index()\n",
    "\n",
    "            nNum = len(pd_as_orders_sku_year_month_eligible_requested_monthly_slice)\n",
    "            print(\"(AS_ORDERS) # OF REQUESTED RDD ELIGIBLE EXAMPLES FOR YEAR \" + str(year) + \" AND MONTH \" + str(month) + \" IS \" + str(nNum))\n",
    "\n",
    "            #\n",
    "            blockSize = 25\n",
    "            m = 0 \n",
    "            block_index = 0\n",
    "            as_orders_rdd_customers[zero_layer_index] = {}\n",
    "\n",
    "            while m <= nNum:\n",
    "\n",
    "                as_orders_rdd_customers_list = []\n",
    "\n",
    "                pd_as_orders_sku_year_month_eligible_requested_monthly_slice_block = pd_as_orders_sku_year_month_eligible_requested_monthly_slice.iloc[m:min(nNum,m+blockSize),:]\n",
    "                product_list = pd_as_orders_sku_year_month_eligible_requested_monthly_slice_block[\"PRODUCT_PART_NUMBER\"].to_list()\n",
    "                df_as_orders_data_slice_block = df_as_orders_data_slice.filter(F.col(\"PRODUCT_PART_NUMBER\").isin(product_list))\n",
    "\n",
    "                start_time = time.time()\n",
    "                print(\"AS_ORDERS OF \" + zero_layer_index + \" BLOCK HAS \" + str(df_as_orders_data_slice_block.count()))\n",
    "                print(\"--- %s seconds --- \" + str(time.time() - start_time))\n",
    "\n",
    "                # Cache the data for faster computation\n",
    "                df_as_orders_data_slice_block.persist()\n",
    "              \n",
    "                # Get RDD relevant customers for this block of requested examples \n",
    "                start_time_0 = time.time()\n",
    "                start_time_00 = time.time()\n",
    "\n",
    "                pd_as_orders_data_sku_year_month_dict = {}\n",
    "                pd_as_orders_cutoff_point_detection_dict = {}\n",
    "                pd_as_orders_cutoff_points_dict = {}\n",
    "\n",
    "                for ix0, row0 in pd_as_orders_sku_year_month_eligible_requested_monthly_slice.iterrows():\n",
    "\n",
    "                    sku = row0[\"PRODUCT_PART_NUMBER\"]\n",
    "                    first_layer_index = str(sku) + \"_\" + str(year) + \"_\" + str(month)\n",
    "                    df_as_orders_data_sku_year_month, pd_as_orders_data_sku_year_month = as_orders_slice_pandas(df_as_orders_data_slice_block, sku, year, month)\n",
    "                    pd_as_orders_data_sku_year_month_dict[first_layer_index] = pd_as_orders_data_sku_year_month\n",
    "\n",
    "                    pd_as_orders_cutoff_point_detection, pd_as_orders_cutoff_points = find_as_orders_cutoff_dates(pd_as_orders_data_sku_year_month)\n",
    "                    pd_as_orders_cutoff_point_detection_dict[first_layer_index] = pd_as_orders_cutoff_point_detection\n",
    "                    pd_as_orders_cutoff_points_dict[first_layer_index] = pd_as_orders_cutoff_points                \n",
    "\n",
    "                    sufficiency_ratio = 0.5\n",
    "                    initial_window_length = 7\n",
    "                    as_orders_rdd_customers_temp = find_as_orders_rdd_relevant_customers(pd_as_orders_cutoff_points,\n",
    "                                                                                                  pd_as_orders_cutoff_point_detection,\n",
    "                                                                                                  pd_as_orders_data_sku_year_month,\n",
    "                                                                                                  sufficiency_ratio, initial_window_length,\n",
    "                                                                                                  first_layer_index)\n",
    "                    as_orders_rdd_customers_list = [*set(as_orders_rdd_customers_list+as_orders_rdd_customers_temp)]\n",
    "\n",
    "\n",
    "                    if (ix0 > 0) & (ix0 % 10 == 0):\n",
    "                        print(\"(AS_ORDERS) \" + str(ix0/nNum*100) + \"%\" + \" COMPLETED FOR \" + zero_layer_index)\n",
    "                        print(\"--- %s seconds --- \" + str(time.time() - start_time_00))\n",
    "                        start_time_00 = time.time()\n",
    "\n",
    "\n",
    "                print(\"(AS_ORDERS) THE NUM. OF RDD RELEVANT CUSTOMERS IN \" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS \" + str(len(as_orders_rdd_customers_list)))\n",
    "                print(\"--- %s seconds --- \" + str(time.time() - start_time_0))\n",
    "                as_orders_rdd_customers[zero_layer_index][\"BLOCK\" + \"_\" + str(block_index)] = as_orders_rdd_customers_list\n",
    "\n",
    "                # save the results (of the month)\n",
    "                # save_file_name = \"as_orders_rdd_customers_\" + zero_layer_index + \".pkl\"\n",
    "\n",
    "                # with open(save_file_name, \"wb\") as fp:\n",
    "                #    pickle.dump(as_orders_rdd_customers, fp)\n",
    "                #    print(\"RDD CUSTOMERS OF\" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS SAVED.\")\n",
    "\n",
    "                # Get the corresponding relevant portion of CP data\n",
    "                df_cp_block = df_cp.filter(F.col(\"CUSTOMER_ID\").isin(as_orders_rdd_customers_list))\n",
    "                \n",
    "                # Cache the data for faster computation\n",
    "                df_cp_block.persist()\n",
    "\n",
    "                start_time = time.time()\n",
    "                print(\"(AS_ORDERS) THE NUMBER OF RELEVANT CP DATA POINTS OF\" + \" BLOCK \" + str(block_index) + \" OF \" + zero_layer_index + \" IS \" + str(df_cp_block.count()))\n",
    "                print(\"--- %s seconds ---\" + str(time.time() - start_time))\n",
    "                \n",
    "                i = 0\n",
    "                start_time = time.time()\n",
    "                start_time_00 = time.time()\n",
    "                for ix0, row0 in pd_as_orders_sku_year_month_eligible_requested_monthly_slice.iterrows():\n",
    "\n",
    "                    # timer\n",
    "                    start_time_initial = time.time()\n",
    "\n",
    "                    # SKU, Year, Month\n",
    "                    sku = row0[\"PRODUCT_PART_NUMBER\"]\n",
    "                    first_layer_index = str(sku) + \"_\" + str(year) + \"_\" + str(month)\n",
    "\n",
    "                    print(\"--- WORKING ON INDEX: \" + str(i) + \" EXAMPLE NO. \" + first_layer_index + \" ---\")\n",
    "\n",
    "                    # Part 1. As_orders Portion of Instock Value  \n",
    "\n",
    "                    # 1. As_orders slice\n",
    "                    pd_as_orders_data_sku_year_month = pd_as_orders_data_sku_year_month_dict[first_layer_index]\n",
    "                    print(\"AS_ORDERS COUNTS FOR \" + first_layer_index + \" IS \" + str(len(pd_as_orders_data_sku_year_month)))\n",
    "\n",
    "                    # 3. As_orders cutoff Date Data\n",
    "                    pd_as_orders_cutoff_point_detection = pd_as_orders_cutoff_point_detection_dict[first_layer_index]\n",
    "                    pd_as_orders_cutoff_points = pd_as_orders_cutoff_points_dict[first_layer_index]     \n",
    "\n",
    "                    print(\"(AS_ORDERS) CUTOFF POINTS OF \" + first_layer_index + \" IS \")\n",
    "                    print(pd_as_orders_cutoff_points)\n",
    "\n",
    "                    # 4. RDD regression sample construction\n",
    "                    sufficiency_ratio = 0.5 \n",
    "                    initial_window_length = 7\n",
    "                    as_orders_rdd_sample_collection[first_layer_index] = construct_as_orders_rdd_samples(pd_as_orders_cutoff_points, pd_as_orders_cutoff_point_detection, pd_as_orders_data_sku_year_month, sufficiency_ratio, initial_window_length, first_layer_index, df_cp_block)\n",
    "\n",
    "                    # with open('as_orders_rdd_sample_collection_part2.pkl', 'wb') as fp:\n",
    "                    #    pickle.dump(as_orders_rdd_sample_collection, fp)\n",
    "                    #    print('dictionary saved successfully to file')\n",
    "\n",
    "                    # with open('as_orders_rdd_sample_collection_part2.pkl', 'rb') as fp:\n",
    "                    #    as_orders_rdd_sample_collection = pickle.load(fp)\n",
    "                    #    print('Reading the RDD samples')\n",
    "\n",
    "                    # 5. Regression and Result Summaries\n",
    "                    # CP_V3: CP of ALL products OTHER THAN the one of interest\n",
    "                    cp_variable = \"CP_V3\"\n",
    "\n",
    "                    # Polynomial order of running variable\n",
    "                    K = 1\n",
    "                    # Three types: SEPARATE_UNWEIGHTED (Run RDD for each sample with OLS), SEPARATE_WEIGHTED (Run RDD for each sample with WLS), JOINT_WEIGHTED (Run RDD on a combined sample with WLS)\n",
    "                    regression_type = \"JOINT_WEIGHTED\"\n",
    "                    # assign higher weights to data points within 24 hours of the cutoff point\n",
    "                    h = 24\n",
    "                    a1 = 1\n",
    "                    a2 = 2\n",
    "                    bootstrapping_method = True\n",
    "                    boostramp_sample_size = 2500\n",
    "                    if len(as_orders_rdd_sample_collection[first_layer_index]) == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        as_orders_rdd_estimate_collection[first_layer_index] = as_orders_rdd_modeling(as_orders_rdd_sample_collection[first_layer_index], first_layer_index, cp_variable, K, regression_type, h, a1, a2, bootstrapping_method, boostramp_sample_size)\n",
    "                        finalized_as_orders_rdd_estimates[first_layer_index] = as_orders_rdd_result_summary(as_orders_rdd_estimate_collection[first_layer_index], first_layer_index)\n",
    "\n",
    "                    with open(\"finalized_as_orders_rdd_estimates.pkl\", \"wb\") as fp:\n",
    "                        pickle.dump(finalized_as_orders_rdd_estimates, fp)\n",
    "                        print(\"(AS_ORDERS) RDD ESTIMATES dictionary saved successfully to file\")    \n",
    "                    \n",
    "                    print(\"--- %s seconds ---\" + str(time.time() - start_time_00))\n",
    "                    start_time_00 = time.time()\n",
    "                    i = i + 1\n",
    "                \n",
    "                print(\"--- %s seconds ---\" + str(time.time() - start_time))\n",
    "                m = m + blockSize\n",
    "                block_index = block_index + 1\n",
    "\n",
    "# 3. Remediation\n",
    "# 3.1 Clickstream\n",
    "# Step 1. Read from the RDD estimate dictionary into Pandas dataframe\n",
    "with open(\"finalized_clickstream_rdd_estimates.pkl\", \"rb\") as fp:\n",
    "    finalized_clickstream_rdd_estimates = pickle.load(fp)\n",
    "    print(\"(CLICKSTREAM) RDD ESTIMATES SUCCESSFULLY READ\") \n",
    "pd_clickstream_valid_rdd_estimates = generate_pd_valid_clickstream_rdd_estimates(finalized_clickstream_rdd_estimates)\n",
    "\n",
    "# Step 2. Read in the product features data and add features to valid rdd estimate data\n",
    "pd_sku_features = pd.read_csv(\"sku_features.csv\")\n",
    "pd_sku_features.columns = [\"PRODUCT_PART_NUMBER\", \"PRODUCT_NAME\", \"MC1\", \"MC2\", \"MC3\"]\n",
    "pd_clickstream_valid_rdd_estimates_with_sku_features = pd_clickstream_valid_rdd_estimates.merge(pd_sku_features, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\n",
    "\n",
    "# Step 3. Remediation algorithm\n",
    "# 3.1. Create the complete list of values\n",
    "pd_sku_needed = pd.read_csv(\"Aristotle_AB_Experiment_Item_Selection.csv\")\n",
    "pd_sku_needed.columns = [x.upper() for x in pd_sku_needed.columns.to_list()]\n",
    "\n",
    "clickstream_year_list = [2020, 2021, 2022]\n",
    "clickstream_month_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "clickstream_product_list = pd_sku_needed[\"PRODUCT_PART_NUMBER\"].to_list()\n",
    "\n",
    "# 3.2. Create product x time complete set\n",
    "pd_clickstream_time_values, pd_clickstream_product_x_time_with_sku_features = generate_clickstream_productxtime_sets(clickstream_year_list, clickstream_month_list, clickstream_product_list, pd_sku_features)\n",
    "\n",
    "# Step 4. Remediation\n",
    "pd_clickstream_remediated_estimates = generate_as_orders_remediated_estimates(clickstream_year_list, clickstream_month_list, pd_clickstream_time_values, pd_clickstream_valid_rdd_estimates_with_sku_features, pd_clickstream_product_x_time_with_sku_features)\n",
    "\n",
    "\n",
    "# 3.2. As_orders\n",
    "# Step 1. Read from the RDD estimate dictionary into Pandas dataframe\n",
    "pd_as_orders_valid_rdd_estimates = generate_pd_valid_as_orders_rdd_estimates(finalized_as_orders_rdd_estimates)\n",
    "\n",
    "# Step 2. Read in the product features data and add features to valid rdd estimate data\n",
    "# pd_sku_features = pd.read_csv()\n",
    "pd_as_orders_valid_rdd_estimates_with_sku_features = pd_as_orders_valid_rdd_estimates.merge(pd_sku_features, on = [\"PRODUCT_PART_NUMBER\"], how = \"left\")\n",
    "\n",
    "\n",
    "# Step 3. Remediation algorithm\n",
    "# 3.1. Create the complete list of values\n",
    "as_orders_year_list = clickstream_year_list\n",
    "as_orders_month_list = clickstream_month_list\n",
    "as_orders_product_list = clickstream_product_list\n",
    "\n",
    "# 3.2. Create product x time complete set\n",
    "pd_as_orders_time_values, pd_as_orders_product_x_time_with_sku_features = generate_as_orders_productxtime_sets(as_orders_year_list, as_orders_month_list, as_orders_product_list, pd_sku_features)\n",
    "\n",
    "# Step 4. Remediation\n",
    "pd_as_orders_remediated_estimates = generate_as_orders_remediated_estimates(as_orders_year_list, as_orders_month_list, pd_as_orders_time_values, pd_as_orders_valid_rdd_estimates_with_sku_features, pd_as_orders_product_x_time_with_sku_features)\n",
    "                \n",
    "        \n",
    "# Part 4. Two key ingredients for clickstream portion of instock value: non-AS demand share & clickstream counts per unit of non-AS demand\n",
    "# 4.1. Clickstream Counts\n",
    "is_clickstream_counts, oos_clickstream_counts, total_clickstream_counts = clickstream_counts_by_instock_status(pd_clickstream_data_slice)\n",
    "\n",
    "print(\"IS CLICKSTREAM COUNTS FOR \" + first_layer_index + \" IS \" + str(is_clickstream_counts))\n",
    "print(\"OOS CLICKSTREAM COUNTS FOR \" + first_layer_index + \" IS \" + str(oos_clickstream_counts))\n",
    "print(\"TOTAL CLICKSTREAM COUNTS FOR \" + first_layer_index + \" IS \" + str(total_clickstream_counts))\n",
    "\n",
    "# 4.2. CP (orders) data\n",
    "df_cp_sku_month = df_cp.filter( (F.col(\"PRODUCT_PART_NUMBER_IN_CP\") == str(product_part_number_for_sample)) & \\\n",
    "                        (F.col(\"ORDER_PLACED_YEAR_CP\") == year_value_for_sample) & \\\n",
    "                        (F.col(\"ORDER_PLACED_MONTH_CP\") == month_value_for_sample))\n",
    "\n",
    "df_cp_sku_month.persist()\n",
    "\n",
    "# 4.3. Units sold by as status\n",
    "start_time = time.time()\n",
    "non_as_units_sold, as_units_sold, total_units_sold = units_sold_by_as_status(df_cp_sku_month)\n",
    "print(\"NON-AS SOLD UNITS FOR \" + first_layer_index + \" IS \" + str(non_as_units_sold))    \n",
    "print(\"AS SOLD UNITS FOR \" + first_layer_index + \" IS \" + str(as_units_sold))  \n",
    "print(\"TOTAL SOLD UNITS FOR \" + first_layer_index + \" IS \" + str(total_units_sold))  \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 4.4. AS & non-AS demand shares\n",
    "non_as_share = non_as_units_sold/(non_as_units_sold+as_units_sold)\n",
    "as_share =  1 - non_as_share\n",
    "\n",
    "print(\"NON-AS DEMAND SHARE FOR \" + first_layer_index + \" IS \" + str(non_as_share))\n",
    "print(\"AS DEMAND SHARE FOR \" + first_layer_index + \" IS \" + str(as_share))\n",
    "\n",
    "# 4.5. Coefficients: clickstream counts per unit of non-AS demand\n",
    "clickstream_coef = is_clickstream_counts / non_as_units_sold\n",
    "\n",
    "print(\"CLICKSTREAM-NON-AS-DEMAND COEFFICIENT FOR \" + first_layer_index + \" IS \" + str(clickstream_coef))\n",
    "\n",
    "# 4.6. save the two key elements: non-AS demand share & clickstream coefficient\n",
    "non_as_demand_share_collection[first_layer_index] = non_as_share\n",
    "clickstream_non_as_demand_coefficient_collection[first_layer_index] = clickstream_coef\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1d5f2-3ca4-4a1f-a7e8-2a7eb12c76c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac78961-ce0a-4ce7-a8a3-dc21fe848d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
